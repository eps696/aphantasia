{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IllusTrip3D.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "f3Sj0fxmtw6K",
        "YdVubN0vb3TU"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWe1IoH7X35"
      },
      "source": [
        "# IllusTrip: Text to Video 3D\n",
        "\n",
        "Part of [Aphantasia](https://github.com/eps696/aphantasia) suite, made by Vadim Epstein [[eps696](https://github.com/eps696)]  \n",
        "Based on [CLIP](https://github.com/openai/CLIP) + FFT/pixel ops from [Lucent](https://github.com/greentfrapp/lucent). \n",
        "3D part by [deKxi](https://twitter.com/deKxi), based on [AdaBins](https://github.com/shariqfarooq123/AdaBins) depth.  \n",
        "thanks to [Ryan Murdock](https://twitter.com/advadnoun), [Jonathan Fly](https://twitter.com/jonathanfly), [@eduwatch2](https://twitter.com/eduwatch2) for ideas.\n",
        "\n",
        "## Features \n",
        "* continuously processes **multiple sentences** (e.g. illustrating lyrics or poems)\n",
        "* makes **videos**, evolving with pan/zoom/rotate motion\n",
        "* works with [inverse FFT](https://github.com/greentfrapp/lucent/blob/master/lucent/optvis/param/spatial.py) representation of the image or **directly with RGB** pixels (no GANs involved)\n",
        "* generates massive detailed textures (a la deepdream), **unlimited resolution**\n",
        "* optional **depth** processing for 3D look\n",
        "* various CLIP models\n",
        "* can start/resume from an image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytcEMSKBtN-"
      },
      "source": [
        "**Run the cell below after each session restart**\n",
        "\n",
        "Ensure that you're given Tesla T4/P4/P100 GPU, not K80!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title General setup\n",
        "\n",
        "!pip install ftfy==5.8 transformers\n",
        "!pip install gputil ffpb \n",
        "\n",
        "try: \n",
        "  !pip3 install googletrans==3.1.0a0\n",
        "  from googletrans import Translator, constants\n",
        "  translator = Translator()\n",
        "except: pass\n",
        "\n",
        "# !apt-get -qq install ffmpeg\n",
        "work_dir = '/content/illustrip'\n",
        "import os\n",
        "os.makedirs(work_dir, exist_ok=True)\n",
        "%cd $work_dir\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "import PIL\n",
        "from base64 import b64encode\n",
        "import shutil\n",
        "from easydict import EasyDict as edict\n",
        "a = edict()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from IPython.display import HTML, Image, display, clear_output\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import ipywidgets as ipy\n",
        "from google.colab import output, files\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git --no-deps\n",
        "import clip\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "!pip install kornia\n",
        "import kornia\n",
        "!pip install lpips\n",
        "import lpips\n",
        "!pip install PyWavelets==1.1.1\n",
        "!pip install git+https://github.com/fbcotter/pytorch_wavelets\n",
        "\n",
        "%cd /content\n",
        "!rm -rf aphantasia\n",
        "!git clone https://github.com/eps696/aphantasia\n",
        "%cd aphantasia/\n",
        "from clip_fft import to_valid_rgb, fft_image, rfft2d_freqs, img2fft, pixel_image, un_rgb\n",
        "from utils import basename, file_list, img_list, img_read, txt_clean, plot_text, old_torch\n",
        "from utils import slice_imgs, derivat, pad_up_to, slerp, checkout, sim_func, latent_anima\n",
        "import transforms\n",
        "from progress_bar import ProgressIPy as ProgressBar\n",
        "shutil.copy('mask.jpg', work_dir)\n",
        "depth_mask_file = os.path.join(work_dir, 'mask.jpg')\n",
        "\n",
        "clear_output()\n",
        "\n",
        "def save_img(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def makevid(seq_dir, size=None):\n",
        "  char_len = len(basename(img_list(seq_dir)[0]))\n",
        "  out_sequence = seq_dir + '/%0{}d.jpg'.format(char_len)\n",
        "  out_video = seq_dir + '.mp4'\n",
        "  print(".. generating video ..")\n",
        "  !ffmpeg -y -v warning -i $out_sequence -crf 18 $out_video\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\n",
        "\n",
        "# Hardware check\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "import GPUtil as GPU\n",
        "gpu = GPU.getGPUs()[0] # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "!nvidia-smi -L\n",
        "print(\"GPU RAM {0:.0f}MB | Free {1:.0f}MB)\".format(gpu.memoryTotal, gpu.memoryFree))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvpdy8BWGuM",
        "cellView": "form"
      },
      "source": [
        "#@title Load inputs\n",
        "\n",
        "#@markdown **Content** (either type a text string, or upload a text file):\n",
        "content = \"\" #@param {type:\"string\"}\n",
        "upload_texts = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown **Style** (either type a text string, or upload a text file):\n",
        "style = \"\" #@param {type:\"string\"}\n",
        "upload_styles = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown For non-English languages use Google translation:\n",
        "translate = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Resume from the saved `.pt` snapshot, or from an image  \n",
        "#@markdown (resolution settings below will be ignored in this case): \n",
        "\n",
        "if upload_texts:\n",
        "  print('Upload main text file')\n",
        "  uploaded = files.upload()\n",
        "  text_file = list(uploaded)[0]\n",
        "  texts = list(uploaded.values())[0].decode().split('\\n')\n",
        "  texts = [tt.strip() for tt in texts if len(tt.strip())>0 and tt[0] != '#']\n",
        "  print(' main text:', text_file, len(texts), 'lines')\n",
        "  workname = txt_clean(basename(text_file))\n",
        "else:\n",
        "  texts = [content]\n",
        "  workname = txt_clean(content)[:44]\n",
        "\n",
        "if upload_styles:\n",
        "  print('Upload styles text file')\n",
        "  uploaded = files.upload()\n",
        "  text_file = list(uploaded)[0]\n",
        "  styles = list(uploaded.values())[0].decode().split('\\n')\n",
        "  styles = [tt.strip() for tt in styles if len(tt.strip())>0 and tt[0] != '#']\n",
        "  print(' styles:', text_file, len(styles), 'lines')\n",
        "else:\n",
        "  styles = [style]\n",
        "\n",
        "resume = False #@param {type:\"boolean\"}\n",
        "if resume:\n",
        "  print('Upload file to resume from')\n",
        "  resumed = files.upload()\n",
        "  resumed_filename = list(resumed)[0]\n",
        "  resumed_bytes = list(resumed.values())[0]\n",
        "\n",
        "assert len(texts) > 0 and len(texts[0]) > 0, 'No input text[s] found!'\n",
        "tempdir = os.path.join(work_dir, workname)\n",
        "os.makedirs(tempdir, exist_ok=True)\n",
        "print('main dir', tempdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQFGziYKtHSa"
      },
      "source": [
        "**`content`** (what to draw) is your primary input; **`style`** (how to draw) is optional, if you want to separate such descriptions.  \n",
        "If you load text file[s], the imagery will interpolate from line to line (ensure equal line counts for content and style lists, for their accordance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uti6XrqiQumf",
        "cellView": "form"
      },
      "source": [
        "#@title Google Drive [optional]\n",
        "\n",
        "#@markdown Run this cell, if you want to store results on your Google Drive.\n",
        "using_GDrive = True#@param{type:\"boolean\"}\n",
        "if using_GDrive:\n",
        "  import os\n",
        "  from google.colab import drive\n",
        "\n",
        "  if not os.path.isdir('/G/MyDrive'): \n",
        "      drive.mount('/G', force_remount=True)\n",
        "  gdir = '/G/MyDrive'\n",
        "\n",
        "  tempdir = os.path.join(gdir, 'illustrip', workname)\n",
        "  os.makedirs(tempdir, exist_ok=True)\n",
        "  print('main dir', tempdir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64mlBCAYeOrB",
        "cellView": "form"
      },
      "source": [
        "#@title Main settings\n",
        "\n",
        "sideX = 1280 #@param {type:\"integer\"}\n",
        "sideY = 720 #@param {type:\"integer\"}\n",
        "steps = 200 #@param {type:\"integer\"}\n",
        "frame_step = 100 #@param {type:\"integer\"}\n",
        "#@markdown > Config\n",
        "method = 'RGB' #@param ['FFT', 'RGB']\n",
        "model = 'ViT-B/32' #@param ['ViT-B/16', 'ViT-B/32', 'RN101', 'RN50x16', 'RN50x4', 'RN50']\n",
        "\n",
        "# Default settings\n",
        "if method == 'RGB':\n",
        "  align = 'overscan'\n",
        "  colors = 2\n",
        "  contrast = 1.2\n",
        "  sharpness = -1.\n",
        "  aug_noise = 0.\n",
        "  smooth = False\n",
        "else:\n",
        "  align = 'uniform'\n",
        "  colors = 1.8\n",
        "  contrast = 1.1\n",
        "  sharpness = 1.\n",
        "  aug_noise = 2.\n",
        "  smooth = True\n",
        "interpolate_topics = True\n",
        "style_power = 1.\n",
        "samples = 200\n",
        "save_step = 1\n",
        "learning_rate = 1.\n",
        "aug_transform = 'custom'\n",
        "similarity_function = 'cossim'\n",
        "macro = 0.4\n",
        "enforce = 0.\n",
        "expand = 0.\n",
        "zoom = 0.012\n",
        "shift = 10\n",
        "rotate = 0.8\n",
        "distort = 0.3\n",
        "animate_them = True\n",
        "sample_decrease = 1.\n",
        "DepthStrength = 0.\n",
        "\n",
        "print(' loading CLIP model..')\n",
        "model_clip, _ = clip.load(model, jit=old_torch())\n",
        "modsize = model_clip.visual.input_resolution\n",
        "xmem = {'ViT-B/16':0.25, 'RN50':0.5, 'RN50x4':0.16, 'RN50x16':0.06, 'RN101':0.33}\n",
        "if model in xmem.keys():\n",
        "  sample_decrease *= xmem[model]\n",
        "\n",
        "clear_output()\n",
        "print(' using CLIP model', model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIWNmmd5uuSn"
      },
      "source": [
        "**`FFT`** method uses inverse FFT representation of the image. It allows flexible motion, but is either blurry (if smoothed) or noisy (if not).  \n",
        "**`RGB`** method directly optimizes image pixels (without FFT parameterization). It's more clean and stable, when zooming in.  \n",
        "There are few choices for CLIP `model` (results do vary!). I prefer ViT-B/32 for consistency, next best bet is ViT-B/16.  \n",
        "\n",
        "**`steps`** defines the length of animation per text line (multiply it to the inputs line count to get total video duration in frames).  \n",
        "`frame_step` sets frequency of the changes in animation (how many frames between motion keypoints).  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Sj0fxmtw6K"
      },
      "source": [
        "## Other settings [optional]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P88_xcpAIXlq",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to override settings, if needed\n",
        "#@markdown [to roll back defaults, run \"Main settings\" cell again]\n",
        "\n",
        "style_power = 1. #@param {type:\"number\"}\n",
        "overscan = True #@param {type:\"boolean\"}\n",
        "align = 'overscan' if overscan else 'uniform'\n",
        "interpolate_topics = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown > Look\n",
        "colors = 2 #@param {type:\"number\"}\n",
        "contrast = 1.2 #@param {type:\"number\"}\n",
        "sharpness = 0. #@param {type:\"number\"}\n",
        "\n",
        "#@markdown > Training\n",
        "samples = 200 #@param {type:\"integer\"}\n",
        "save_step = 1 #@param {type:\"integer\"}\n",
        "learning_rate = 1. #@param {type:\"number\"}\n",
        "\n",
        "#@markdown > Tricks\n",
        "aug_transform = 'custom' #@param ['elastic', 'custom', 'none']\n",
        "aug_noise = 0. #@param {type:\"number\"}\n",
        "macro = 0.4 #@param {type:\"number\"}\n",
        "enforce = 0. #@param {type:\"number\"}\n",
        "expand = 0. #@param {type:\"number\"}\n",
        "similarity_function = 'cossim' #@param ['cossim', 'spherical', 'mixed', 'angular', 'dot']\n",
        "\n",
        "#@markdown > Motion\n",
        "zoom = 0.012 #@param {type:\"number\"}\n",
        "shift = 10 #@param {type:\"number\"}\n",
        "rotate = 0.8 #@param {type:\"number\"}\n",
        "distort = 0.3 #@param {type:\"number\"}\n",
        "animate_them = True #@param {type:\"boolean\"}\n",
        "smooth = True #@param {type:\"boolean\"}\n",
        "if method == 'RGB': smooth = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYrJTb8xDm9C"
      },
      "source": [
        "`style_power` controls the strength of the style descriptions, comparing to the main input.  \n",
        "`overscan` provides better frame coverage (needed for RGB method).  \n",
        "`interpolate_topics` changes the subjects smoothly, otherwise they're switched by cut, making sharper transitions.  \n",
        "\n",
        "Decrease **`samples`** if you face OOM (it's the main RAM eater), or just to speed up the process (with the cost of quality).  \n",
        "`save_step` defines, how many optimization steps are taken between saved frames. Set it >1 for stronger image processing.   \n",
        "\n",
        "Experimental tricks:  \n",
        "`aug_transform` applies some augmentations, which quite radically change the output of this method (and slow down the process). Try yourself to see which is good for your case. `aug_noise` augmentation [FFT only!] seems to enhance optimization with transforms.  \n",
        "`macro` boosts bigger forms.  \n",
        "`enforce` adds more details by enforcing similarity between two parallel samples.  \n",
        "`expand` boosts diversity (up to irrelevant) by enforcing difference between prev/next samples.  \n",
        "\n",
        "Motion section:\n",
        "`shift` is in pixels, `rotate` in degrees. The values will be used as limits, if you mark `animate_them`.  \n",
        "\n",
        "`smooth` reduces blinking, but induces motion blur with subtle screen-fixed patterns (valid only for FFT method, disabled for RGB).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdVubN0vb3TU"
      },
      "source": [
        "## Add 3D depth [optional]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl-rm1Nm03lK",
        "cellView": "form"
      },
      "source": [
        "### deKxi:: This whole cell contains most of whats needed, \n",
        "# with just a few changes to hook it up via frame_transform \n",
        "# (also glob_step now as global var)\n",
        "\n",
        "# I highly recommend performing the frame transformations and depth *after* saving,\n",
        "# (or just the depth warp if you prefer to keep the other affines as they are)\n",
        "# from my testing it reduces any noticeable stretching and allows the new areas\n",
        "# revealed from the changed perspective to be filled/detailed \n",
        "\n",
        "# pretrained models: Nyu is much better but Kitti is an option too\n",
        "depth_model = 'nyu' # @ param [\"nyu\",\"kitti\"]\n",
        "workdir_depth = \"/content\"\n",
        "DepthStrength = 0.01 #@param{type:\"number\"}\n",
        "MaskBlurAmt = 33 #@param{type:\"integer\"}\n",
        "\n",
        "if DepthStrength > 0:\n",
        "  depthdir = os.path.join(tempdir, 'depth')\n",
        "  os.makedirs(depthdir, exist_ok=True)\n",
        "  print('depth dir', depthdir)\n",
        "\n",
        "#@markdown NB: depth computing may take up to ~3x more time. Read the comments inside for more info. \n",
        "\n",
        "#@markdown Courtesy of [deKxi](https://twitter.com/deKxi)\n",
        "\n",
        "# Some useful misc funcs used\n",
        "ToImage  = T.ToPILImage()\n",
        "\n",
        "def numpy2tensor(imgArray):\n",
        "  im = torch.unsqueeze(torchvision.transforms.ToTensor()(imgArray), 0)\n",
        "  return im\n",
        "\n",
        "def triangle_blur(x, kernel_size=3, pow=1.0):\n",
        "  padding = (kernel_size-1) // 2\n",
        "  b,c,h,w = x.shape\n",
        "  kernel = torch.linspace(-1,1,kernel_size+2)[1:-1].abs().neg().add(1).reshape(1,1,1,kernel_size).pow(pow).cuda()\n",
        "  kernel = kernel / kernel.sum()\n",
        "  x = x.reshape(b*c,1,h,w)\n",
        "  x = F.pad(x, (padding,padding,padding,padding), mode='reflect')\n",
        "  x = F.conv2d(x, kernel)\n",
        "  x = F.conv2d(x, kernel.permute(0,1,3,2))\n",
        "  x = x.reshape(b,c,h,w)\n",
        "  return x\n",
        "\n",
        "# Imports, some necessary and some just handy\n",
        "###############\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "if not os.path.exists(os.path.join(workdir_depth, 'AdaBins')):\n",
        "  %cd $workdir_depth\n",
        "  !git clone https://github.com/shariqfarooq123/AdaBins.git\n",
        "workdir_depth = os.path.join(workdir_depth, \"AdaBins\")\n",
        "%cd $workdir_depth\n",
        "if not os.path.exists('pretrained'):\n",
        "  !mkdir pretrained\n",
        "if not os.path.exists(os.path.join(workdir_depth, \"pretrained/AdaBins_nyu.pt\")):\n",
        "  !gdown https://drive.google.com/uc?id=1lvyZZbC9NLcS8a__YPcUP7rDiIpbRpoF\n",
        "  if not os.path.exists('AdaBins_nyu.pt'):\n",
        "    !wget https://www.dropbox.com/s/r1zyrfypixviwa8/AdaBins_nyu.pt\n",
        "  !mv AdaBins_nyu.pt pretrained/AdaBins_nyu.pt\n",
        "# if depth_model=='kitti' and not os.path.exists(os.path.join(workdir_depth, \"pretrained/AdaBins_kitti.pt\")):\n",
        "  # !gdown https://drive.google.com/uc?id=1HMgff-FV6qw1L0ywQZJ7ECa9VPq1bIoj\n",
        "  # !mv AdaBins_kitti.pt pretrained/AdaBins_kitti.pt\n",
        "\n",
        "############ Mask is for blending multi-crop depth \n",
        "global mask_blurred\n",
        "\n",
        "masksize = (830, 500) # I've hardcoded this but it doesn't have to be this exact number, this is just the max for what works at 16:9 for each crop\n",
        "mask = cv2.imread(depth_mask_file, cv2.IMREAD_GRAYSCALE)\n",
        "mask = cv2.resize(mask, masksize)\n",
        "ch = sideY//2\n",
        "cw = sideX//2\n",
        "mask_blur  = cv2.GaussianBlur(mask,(MaskBlurAmt,MaskBlurAmt),0)\n",
        "mask_blurred = cv2.resize(mask_blur,(cw,ch)) / 255.\n",
        "############\n",
        "\n",
        "from infer import InferenceHelper\n",
        "infer_helper = InferenceHelper(dataset='nyu')\n",
        "# You can adjust AdaBins' internal depth max and min here, but unsure if it makes a huge difference - haven't tested it too much yet\n",
        "#infer_helper.max_depth = infer_helper.max_depth * 50\n",
        "#infer_helper.min_depth = infer_helper.min_depth * 1\n",
        "\n",
        "%cd /content/aphantasia/\n",
        "def depthwarp(img, strength=0, rescale=0, midpoint=0.5, depth_origin=(0,0), clip_range=0, save_depth=True, multicrop=True):\n",
        "  if strength==0: return img\n",
        "  img2 = img.clone().detach() # Most of the pre-inference operations will take place on a dummy cloned tensor for simplicity sake \n",
        "  \n",
        "  # Blurring first can somewhat mitigate the inherent noise from pixelgen method. Feel free to change these values if the depthmap is unsatisfactory\n",
        "  img2 = torch.lerp(img2, triangle_blur((img2), 5, 2), 0.5)\n",
        "\n",
        "  _, _, H, W = img2.shape\n",
        "  # This will define the centre/origin point for the depth extrusion\n",
        "  centre = torch.as_tensor([depth_origin[0],depth_origin[1]]).cpu()\n",
        "\n",
        "  # Converting the tensor to image in order to perform inference, probably a cleaner way to do this though as this was quick and dirty\n",
        "  par, imag, _ = pixel_image([1,3,H,W], resume=img2)\n",
        "  img2 = to_valid_rgb(imag, colors=colors)()\n",
        "  img2 = img2.detach().cpu().numpy()[0]\n",
        "  img2 = (np.transpose(img2, (1,2,0))) # convert image back to Height,Width,Channels\n",
        "  img2 = np.clip(img2*255, 0, 255).astype(np.uint8)\n",
        "  image = ToImage(img2)\n",
        "  del img2\n",
        "\n",
        "  # Resize down for inference\n",
        "  if H < W: # 500p on either dimension was the limit I found for AdaBins\n",
        "    r = 500 / float(H)\n",
        "    dim = (int(W * r), 500)\n",
        "  else:\n",
        "    r = 500 / float(W)\n",
        "    dim = (500, int(H * r))\n",
        "  image = image.resize(dim,3)\n",
        "\n",
        "  bin_centres, predicted_depth = infer_helper.predict_pil(image)   \n",
        "  \n",
        "  # Resize back to original before (optionally) adding the cropped versions\n",
        "  predicted_depth = cv2.resize(predicted_depth[0][0],(W,H))\n",
        "\n",
        "  if multicrop: \n",
        "    # This code is very jank as I threw it together as a quick proof-of-concept, and it miraculously worked \n",
        "    # There's very likely to be some improvements that can be made\n",
        "\n",
        "    clone = predicted_depth.copy()\n",
        "    # Splitting the image into separate crops, probably inefficiently\n",
        "    TL = torchvision.transforms.functional.crop(image.resize((H,W),3), top=0, left=0, height=cw, width=ch).resize(dim,3)\n",
        "    TR = torchvision.transforms.functional.crop(image.resize((H,W),3), top=0, left=ch, height=cw, width=ch).resize(dim,3)\n",
        "    BL = torchvision.transforms.functional.crop(image.resize((H,W),3), top=cw, left=0, height=cw, width=ch).resize(dim,3)\n",
        "    BR = torchvision.transforms.functional.crop(image.resize((H,W),3), top=cw, left=ch, height=cw, width=ch).resize(dim,3)\n",
        "\n",
        "    # Inference on crops\n",
        "    _, predicted_TL = infer_helper.predict_pil(TL)\n",
        "    _, predicted_TR = infer_helper.predict_pil(TR)\n",
        "    _, predicted_BL = infer_helper.predict_pil(BL)\n",
        "    _, predicted_BR = infer_helper.predict_pil(BR)\n",
        "\n",
        "    # Rescale will increase per object depth difference, but may cause more depth fluctuations if set too high\n",
        "    # This likely results in the depth map being less \"accurate\" to any real world units.. not that it was particularly in the first place lol\n",
        "    if rescale != 0:\n",
        "      # Histogram equalize requires a range of 0-255, but I'm recombining later in 0-1 hence this mess\n",
        "      TL = cv2.addWeighted(cv2.equalizeHist(predicted_TL.astype(np.uint8) * 255) / 255., 1-rescale, \n",
        "                      predicted_TL.astype(np.uint8),rescale,0)\n",
        "      TR = cv2.addWeighted(cv2.equalizeHist(predicted_TR.astype(np.uint8) * 255) / 255., 1-rescale, \n",
        "                      predicted_TR.astype(np.uint8),rescale,0)\n",
        "      BL = cv2.addWeighted(cv2.equalizeHist(predicted_BL.astype(np.uint8) * 255) / 255., 1-rescale, \n",
        "                      predicted_BL.astype(np.uint8),rescale,0)\n",
        "      BR = cv2.addWeighted(cv2.equalizeHist(predicted_BR.astype(np.uint8) * 255) / 255., 1-rescale, \n",
        "                      predicted_BR.astype(np.uint8),rescale,0)\n",
        "    # Combining / blending the crops with the original, quite a janky solution admittedly\n",
        "    TL = clone[0: ch, 0: cw] * (1 - mask_blurred) + cv2.resize(predicted_TL[0][0],(cw,ch)) * mask_blurred\n",
        "    TR = clone[0: ch, cw: cw+cw] * (1 - mask_blurred) + cv2.resize(predicted_TR[0][0],(cw,ch)) * mask_blurred\n",
        "    BL = clone[ch: ch+ch, 0: cw] * (1 - mask_blurred) + cv2.resize(predicted_BL[0][0],(cw,ch)) * mask_blurred\n",
        "    BR = clone[ch: ch+ch, cw: cw+cw] * (1 - mask_blurred) + cv2.resize(predicted_BR[0][0],(cw,ch)) * mask_blurred\n",
        "\n",
        "    # If you wish to display the depth map for each crop and for the merged version, uncomment these\n",
        "    #with outpic:\n",
        "      #plt.imshow(TL, cmap='plasma')\n",
        "      #plt.show()\n",
        "      #plt.imshow(TR, cmap='plasma')\n",
        "      #plt.show()\n",
        "      #plt.imshow(BL, cmap='plasma')\n",
        "      #plt.show()\n",
        "      #plt.imshow(BR, cmap='plasma')\n",
        "      #plt.show()\n",
        "    clone[0: ch, 0: cw] = TL\n",
        "    clone[0: ch, cw: cw+cw] = TR\n",
        "    clone[ch: ch+ch, 0: cw] = BL\n",
        "    clone[ch: ch+ch, cw: cw+cw] = BR\n",
        "    \n",
        "    # I'm just multiplying the depths currently, but performing a pixel average is possibly a better idea\n",
        "    predicted_depth = predicted_depth * clone\n",
        "    predicted_depth /= np.max(predicted_depth) # Renormalize so we don't blow the image out of range\n",
        "\n",
        "    # Display combined end result\n",
        "    with outpic: \n",
        "      plt.imshow(predicted_depth, cmap='plasma')\n",
        "      plt.show()\n",
        "  \n",
        "  #### Generating a new depth map on each frame can sometimes cause temporal depth fluctuations and \"popping\". \n",
        "  #### This part is just some of my experiments trying to mitigate that\n",
        "  # Dividing by average\n",
        "  #ave = np.mean(predicted_depth)\n",
        "  #predicted_depth = np.true_divide(predicted_depth, ave)\n",
        "  # Clipping the very end values that often throw the histogram equalize balance off which can mitigate rescales negative effects\n",
        "  gmin = np.percentile(predicted_depth,0+clip_range)#5\n",
        "  gmax = np.percentile(predicted_depth,100-clip_range)#8\n",
        "  clipped = np.clip(predicted_depth, gmin, gmax)\n",
        "\n",
        "  # Depth is reversed, hence the \"1 - x\"\n",
        "  predicted_depth = (1 - ((clipped - gmin) / (gmax - gmin))) * 255\n",
        "\n",
        "  # Rescaling helps emphasise the depth difference but is less \"accurate\". The amount gets mixed in via lerp\n",
        "  if rescale != 0:\n",
        "    rescaled = numpy2tensor(cv2.equalizeHist(predicted_depth.astype(np.uint8)))\n",
        "    rescaled = torchvision.transforms.Resize((H,W))(rescaled.cuda())\n",
        "  \n",
        "  # Renormalizing again before converting back to tensor\n",
        "  predicted_depth = predicted_depth.astype(np.uint8) / np.max(predicted_depth.astype(np.uint8))\n",
        "  dtensor = numpy2tensor(PIL.Image.fromarray(predicted_depth)).cuda()\n",
        "  #dtensor = torchvision.transforms.Resize((H,W))(dtensor.cuda())\n",
        "\n",
        "  if rescale != 0: # Mixin amount for rescale, from 0-1\n",
        "    dtensor = torch.lerp(dtensor, rescaled, rescale)\n",
        "\n",
        "  if save_depth: # Save depth map out, currently its as its own image but it could just be added as an alpha channel to main image\n",
        "    global glob_step\n",
        "    saveddepth = dtensor.detach().clone().cpu().squeeze(0)\n",
        "    save_img(saveddepth, os.path.join(depthdir, '%05d.jpg' % glob_step))\n",
        "\n",
        "  dtensor = dtensor.squeeze(0)\n",
        "\n",
        "  # Building the coordinates, most of this is on CPU since it uses numpy\n",
        "  xx = torch.linspace(-1, 1, W)\n",
        "  yy = torch.linspace(-1, 1, H)\n",
        "  gy, gx = torch.meshgrid(yy, xx)\n",
        "  grid = torch.stack([gx, gy], dim=-1).cpu()\n",
        "  d = (centre-grid).cpu()\n",
        "  # Simple lens distortion that can help mitigate the \"stretching\" that appears in the periphery\n",
        "  lens_distortion = torch.sqrt((d**2).sum(axis=-1)).cpu()\n",
        "  #grid2 = torch.stack([gx, gy], dim=-1)\n",
        "  d_sum = dtensor[0]\n",
        "\n",
        "  # Adjust midpoint / move direction\n",
        "  d_sum = (d_sum - (torch.max(d_sum) * midpoint)).cpu()\n",
        "  \n",
        "  # Apply the depth map (and lens distortion) to the grid coordinates\n",
        "  grid += d * d_sum.unsqueeze(-1) * strength\n",
        "  del image, bin_centres, predicted_depth\n",
        "\n",
        "  # Perform the depth warp\n",
        "  img = torch.nn.functional.grid_sample(img, grid.unsqueeze(0).cuda(), align_corners=True, padding_mode='reflection')\n",
        "  \n",
        "  # Reset and perform the lens distortion warp (with reduced strength)\n",
        "  grid = torch.stack([gx, gy], dim=-1).cpu()\n",
        "  grid += d * lens_distortion.unsqueeze(-1) * (strength*0.31)\n",
        "  img = torch.nn.functional.grid_sample(img, grid.unsqueeze(0).cuda(), align_corners=True, padding_mode='reflection')\n",
        "\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZFuwNux8oEg"
      },
      "source": [
        "## Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s",
        "cellView": "form"
      },
      "source": [
        "#@title Generate\n",
        "\n",
        "if aug_transform == 'elastic':\n",
        "  trform_f = transforms.transforms_elastic\n",
        "  sample_decrease *= 0.95\n",
        "elif aug_transform == 'custom':\n",
        "  trform_f = transforms.transforms_custom  \n",
        "  sample_decrease *= 0.95\n",
        "else:\n",
        "  trform_f = transforms.normalize()\n",
        "\n",
        "if enforce != 0:\n",
        "  sample_decrease *= 0.5\n",
        "\n",
        "samples = int(samples * sample_decrease)\n",
        "print(' using %s method, %d samples' % (method, samples))\n",
        "\n",
        "if translate:\n",
        "  translator = Translator()\n",
        "\n",
        "def enc_text(txt):\n",
        "  if translate:\n",
        "    txt = translator.translate(txt, dest='en').text\n",
        "  emb = model_clip.encode_text(clip.tokenize(txt).cuda()[:77])\n",
        "  return emb.detach().clone()\n",
        "\n",
        "# Encode inputs\n",
        "count = 0 # max count of texts and styles\n",
        "key_txt_encs = [enc_text(txt) for txt in texts]\n",
        "count = max(count, len(key_txt_encs))\n",
        "key_styl_encs = [enc_text(style) for style in styles]\n",
        "count = max(count, len(key_styl_encs))\n",
        "assert count > 0, \"No inputs found!\"\n",
        "\n",
        "# !rm -rf $tempdir\n",
        "# os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "opt_steps = steps * save_step # for optimization\n",
        "glob_steps = count * steps # saving\n",
        "if glob_steps == frame_step: frame_step = glob_steps // 2 # otherwise no motion\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "if method == 'RGB':\n",
        "\n",
        "  if resume:\n",
        "    img_in = imageio.imread(resumed_bytes) / 255.\n",
        "    params_tmp = torch.Tensor(img_in).permute(2,0,1).unsqueeze(0).float().cuda()\n",
        "    params_tmp = un_rgb(params_tmp, colors=1.)\n",
        "    sideY, sideX = img_in.shape[0], img_in.shape[1]\n",
        "  else:\n",
        "    params_tmp = torch.randn(1, 3, sideY, sideX).cuda() # * 0.01\n",
        "\n",
        "else: # FFT\n",
        "\n",
        "  if resume:\n",
        "    if os.path.splitext(resumed_filename)[1].lower()[1:] in ['jpg','png','tif','bmp']:\n",
        "      img_in = imageio.imread(resumed_bytes)\n",
        "      params_tmp = img2fft(img_in, 1.5, 1.) * 2.\n",
        "    else:\n",
        "      params_tmp = torch.load(io.BytesIO(resumed_bytes))\n",
        "      if isinstance(params_tmp, list): params_tmp = params_tmp[0]\n",
        "    params_tmp = params_tmp.cuda()\n",
        "    sideY, sideX = params_tmp.shape[2], (params_tmp.shape[3]-1)*2\n",
        "  else:\n",
        "    params_shape = [1, 3, sideY, sideX//2+1, 2]\n",
        "    params_tmp = torch.randn(*params_shape).cuda() * 0.01\n",
        "  \n",
        "params_tmp = params_tmp.detach()\n",
        "# function() = torch.transformation(linear)\n",
        "\n",
        "# animation\n",
        "if animate_them:\n",
        "  if method == 'RGB':\n",
        "    m_scale = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[-0.3])\n",
        "    m_scale = 1 + (m_scale + 0.3) * zoom # only zoom in\n",
        "  else:\n",
        "    m_scale = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.6])\n",
        "    m_scale = 1 - (m_scale-0.6) * zoom # ping pong\n",
        "  m_shift = latent_anima([2], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.5,0.5])\n",
        "  m_angle = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.5])\n",
        "  m_shear = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.5])\n",
        "  m_shift = (m_shift-0.5) * shift   * abs(m_scale-1.) / zoom\n",
        "  m_angle = (m_angle-0.5) * rotate  * abs(m_scale-1.) / zoom\n",
        "  m_shear = (m_shear-0.5) * distort * abs(m_scale-1.) / zoom\n",
        "\n",
        "def get_encs(encs, num):\n",
        "  cnt = len(encs)\n",
        "  if cnt == 0: return []\n",
        "  enc_1 = encs[min(num,   cnt-1)]\n",
        "  enc_2 = encs[min(num+1, cnt-1)]\n",
        "  return slerp(enc_1, enc_2, opt_steps)\n",
        "\n",
        "def frame_transform(img, size, angle, shift, scale, shear):\n",
        "  ### deKxi:: Performing depth warp first so the standard affine zoom can remove any funkiness at the edges\n",
        "  if DepthStrength > 0: \n",
        "    # Some quick sine animating, didnt bother hooking them up to latent_anima since I replaced it with a different animation method in my own version\n",
        "    # d X/Y define the origin point of the depth warp, effectively a \"3D pan zoom\". Range is '-1 -> 1', with the ends being quite extreme \n",
        "    dX = 0.45 * float(math.sin(((glob_step % 114)/114) * math.pi * 2))\n",
        "    dY = -0.45 * float(math.sin(((glob_step % 166)/166) * math.pi * 2))\n",
        "    # # Midpointoffset == Movement Direction: \n",
        "    #   1 == everything recedes away, 0 == everything moves towards. \n",
        "    #   (and oscillating/animating this value is quite visually pleasing IMO)\n",
        "    midpointOffset = 0.5 + (0.5 * float(math.sin(((glob_step % 70)/70) * math.pi * 2))) \n",
        "    depthOrigin = (dX, dY)\n",
        "    # Perform the warp\n",
        "    depthAmt = DepthStrength*scale # I like to multiply by zoom amount\n",
        "    # It might be worth combining shift with dX/Y change as well\n",
        "\n",
        "    # Rescale combined with clipping end values can improve temporal consistency of depth map\n",
        "    # but generally speaking the technique itself inherently has that fluctuation due to\n",
        "    # independently inferred frames. Best combined with a low depth strength to effectively\n",
        "    # average out the fluctuates\n",
        "\n",
        "    # ^ Performing a batch of depth inference with some augments and averaging could help\n",
        "    # alleviate this, but could end up being performance heavy. Was on my test todo list\n",
        "    # prior to posting publically, but haven't had the time to try it yet\n",
        "    img = depthwarp(img, strength=depthAmt, rescale=0.5, midpoint=midpointOffset, depth_origin=depthOrigin, clip_range=2, save_depth=True, multicrop=True)\n",
        "\n",
        "  if old_torch(): # 1.7.1\n",
        "    img = T.functional.affine(img, angle, shift, scale, shear, fillcolor=0, resample=PIL.Image.BILINEAR)\n",
        "    img = T.functional.center_crop(img, size)\n",
        "    img = pad_up_to(img, size)\n",
        "  else: # 1.8+\n",
        "    img = T.functional.affine(img, angle, shift, scale, shear, fill=0, interpolation=T.InterpolationMode.BILINEAR)\n",
        "    img = T.functional.center_crop(img, size) # on 1.8+ also pads\n",
        "  return img\n",
        "\n",
        "prev_enc = 0\n",
        "def process(num):\n",
        "  global params_tmp, opt_state, params, image_f, optimizer, pbar\n",
        "\n",
        "  if interpolate_topics:\n",
        "    txt_encs  = get_encs(key_txt_encs,  num)\n",
        "    styl_encs = get_encs(key_styl_encs, num)\n",
        "  else:\n",
        "    txt_encs  = [key_txt_encs[min(num,  len(key_txt_encs)-1)][0]]  * opt_steps if len(key_txt_encs)  > 0 else []\n",
        "    styl_encs = [key_styl_encs[min(num, len(key_styl_encs)-1)][0]] * opt_steps if len(key_styl_encs) > 0 else []\n",
        "\n",
        "  if len(texts)  > 0: print(' ref text: ',  texts[min(num, len(texts)-1)][:80])\n",
        "  if len(styles) > 0: print(' ref style: ', styles[min(num, len(styles)-1)][:80])\n",
        "\n",
        "  for ii in range(opt_steps):\n",
        "    global glob_step ### deKxi:: Making this global since I use it everywhere, but especially for saving the depth images out\n",
        "    glob_step = num * steps + ii // save_step # saving/transforming\n",
        "    loss = 0\n",
        "\n",
        "    txt_enc = txt_encs[ii].unsqueeze(0)\n",
        "\n",
        "    # motion: transform frame, reload params\n",
        "    if ii % save_step == 0:\n",
        "\n",
        "      # get encoded inputs\n",
        "      txt_enc  = txt_encs[ii % len(txt_encs)].unsqueeze(0)   if len(txt_encs)  > 0 else None\n",
        "      styl_enc = styl_encs[ii % len(styl_encs)].unsqueeze(0) if len(styl_encs) > 0 else None\n",
        "            \n",
        "      # render test frame\n",
        "      h, w = sideY, sideX\n",
        "      \n",
        "      # transform frame for motion\n",
        "      scale =       m_scale[glob_step]    if animate_them else 1-zoom\n",
        "      trans = tuple(m_shift[glob_step])   if animate_them else [0, shift]\n",
        "      angle =       m_angle[glob_step][0] if animate_them else rotate\n",
        "      shear =       m_shear[glob_step][0] if animate_them else distort\n",
        "\n",
        "      if method == 'RGB':\n",
        "        img_tmp = frame_transform(params_tmp, (h,w), angle, trans, scale, shear)\n",
        "        params, image_f, _ = pixel_image([1,3,h,w], resume=img_tmp)\n",
        "\n",
        "      else: # FFT\n",
        "        if old_torch(): # 1.7.1\n",
        "          img_tmp = torch.irfft(params_tmp, 2, normalized=True, signal_sizes=(h,w))\n",
        "          img_tmp = frame_transform(img_tmp, (h,w), angle, trans, scale, shear)\n",
        "          params_tmp = torch.rfft(img_tmp, 2, normalized=True)\n",
        "        else: # 1.8+\n",
        "          if type(params_tmp) is not torch.complex64:\n",
        "            params_tmp = torch.view_as_complex(params_tmp)\n",
        "          img_tmp = torch.fft.irfftn(params_tmp, s=(h,w), norm='ortho')\n",
        "          img_tmp = frame_transform(img_tmp, (h,w), angle, trans, scale, shear)\n",
        "          params_tmp = torch.fft.rfftn(img_tmp, s=[h,w], dim=[2,3], norm='ortho')\n",
        "          params_tmp = torch.view_as_real(params_tmp)\n",
        "\n",
        "        params, image_f, _ = fft_image([1,3,h,w], resume=params_tmp, sd=1.)\n",
        "\n",
        "      image_f = to_valid_rgb(image_f, colors=colors)\n",
        "      del img_tmp\n",
        "      optimizer = torch.optim.Adam(params, learning_rate)\n",
        "      # optimizer = torch.optim.AdamW(params, learning_rate, weight_decay=0.01, amsgrad=True)\n",
        "      if smooth is True and num + ii > 0:\n",
        "        optimizer.load_state_dict(opt_state)\n",
        "\n",
        "    noise = aug_noise * (torch.rand(1, 1, *params[0].shape[2:4], 1)-0.5).cuda() if aug_noise > 0 else 0.\n",
        "    img_out = image_f(noise)\n",
        "    img_sliced = slice_imgs([img_out], samples, modsize, trform_f, align, macro)[0]\n",
        "    out_enc = model_clip.encode_image(img_sliced)\n",
        "\n",
        "    if method == 'RGB': # empirical hack\n",
        "      loss += 1.5 * abs(img_out.mean((2,3)) - 0.45).mean() # fix brightness\n",
        "      loss += 1.5 * abs(img_out.std((2,3)) - 0.17).sum() # fix contrast\n",
        "\n",
        "    if txt_enc is not None:\n",
        "      loss -= sim_func(txt_enc, out_enc, similarity_function)\n",
        "    if styl_enc is not None:\n",
        "      loss -= style_power * sim_func(styl_enc, out_enc, similarity_function)\n",
        "    if sharpness != 0: # mode = scharr|sobel|naive\n",
        "      loss -= sharpness * derivat(img_out, mode='naive')\n",
        "      # loss -= sharpness * derivat(img_sliced, mode='scharr')\n",
        "    if enforce != 0:\n",
        "      img_sliced = slice_imgs([image_f(noise)], samples, modsize, trform_f, align, macro)[0]\n",
        "      out_enc2 = model_clip.encode_image(img_sliced)\n",
        "      loss -= enforce * sim_func(out_enc, out_enc2, similarity_function)\n",
        "      del out_enc2; torch.cuda.empty_cache()\n",
        "    if expand > 0:\n",
        "      global prev_enc\n",
        "      if ii > 0:\n",
        "        loss += expand * sim_func(prev_enc, out_enc, similarity_function)\n",
        "      prev_enc = out_enc.detach().clone()\n",
        "    del img_out, img_sliced, out_enc; torch.cuda.empty_cache()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if ii % save_step == save_step-1:\n",
        "      params_tmp = params[0].detach().clone()\n",
        "      if smooth is True:\n",
        "        opt_state = optimizer.state_dict()\n",
        "\n",
        "    if ii % save_step == 0:\n",
        "      with torch.no_grad():\n",
        "        img = image_f(contrast=contrast).cpu().numpy()[0]\n",
        "      save_img(img, os.path.join(tempdir, '%05d.jpg' % glob_step))\n",
        "      outpic.clear_output()\n",
        "      with outpic:\n",
        "        display(Image('result.jpg'))\n",
        "      del img\n",
        "      pbar.upd()\n",
        "  \n",
        "  params_tmp = params[0].detach().clone()\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "pbar = ProgressBar(glob_steps)\n",
        "for i in range(count):\n",
        "  process(i)\n",
        "\n",
        "HTML(makevid(tempdir))\n",
        "files.download(tempdir + '.mp4')\n",
        "\n",
        "## deKxi: downloading depth video\n",
        "if DepthStrength > 0:\n",
        "  HTML(makevid(depthdir))\n",
        "  files.download(depthdir + '.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsehQRircaw7"
      },
      "source": [
        "If video is not auto-downloaded after generation (for whatever reason), run this cell to do that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0BZVNi8cZUP"
      },
      "source": [
        "files.download(tempdir + '.mp4')\n",
        "if DepthStrength > 0:\n",
        "  files.download(depthdir + '.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}