{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Illustra.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWe1IoH7X35"
      },
      "source": [
        "# Illustra: Multi-text to Image\n",
        "\n",
        "Part of [Aphantasia](https://github.com/eps696/aphantasia) suite, made by Vadim Epstein [[eps696](https://github.com/eps696)]  \n",
        "Based on [CLIP](https://github.com/openai/CLIP) + FFT from [Lucent](https://github.com/greentfrapp/lucent).  \n",
        "thanks to [Ryan Murdock](https://twitter.com/advadnoun), [Jonathan Fly](https://twitter.com/jonathanfly), [@eduwatch2](https://twitter.com/eduwatch2) for ideas.\n",
        "\n",
        "## Features \n",
        "* **continuously processes phrase lists** (e.g. illustrating lyrics)\n",
        "* generates massive detailed high res imagery, a la deepdream\n",
        "* directly parameterized with [FFT](https://github.com/greentfrapp/lucent/blob/master/lucent/optvis/param/spatial.py) (no pretrained GANs)\n",
        "* various CLIP models (including multi-language from [SBERT](https://sbert.net))\n",
        "* saving/loading FFT snapshots to resume processing\n",
        "* separate text prompt for image style\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytcEMSKBtN-"
      },
      "source": [
        "**Run the cell below after each session restart**\n",
        "\n",
        "Mark `resume` and upload `.pt` file, if you're resuming from the saved params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title General setup\n",
        "\n",
        "!pip install ftfy==5.8 transformers==4.6.0\n",
        "!pip install gputil ffpb \n",
        "\n",
        "!apt-get -qq install ffmpeg\n",
        "from google.colab import drive\n",
        "drive.mount('/G', force_remount=True)\n",
        "# gdir = !ls /G/\n",
        "# gdir = '/G/%s/' % str(gdir[0])\n",
        "gdir = '/G/MyDrive/'\n",
        "%cd $gdir\n",
        "work_dir = 'illustra'\n",
        "import os\n",
        "work_dir = os.path.join(gdir, work_dir)\n",
        "os.makedirs(work_dir, exist_ok=True)\n",
        "%cd $work_dir\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "import PIL\n",
        "from base64 import b64encode\n",
        "import shutil\n",
        "# import moviepy, moviepy.editor\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from IPython.display import HTML, Image, display, clear_output\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import ipywidgets as ipy\n",
        "from google.colab import output, files\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git --no-deps\n",
        "import clip\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "!pip install kornia\n",
        "import kornia\n",
        "!pip install lpips\n",
        "import lpips\n",
        "!pip install PyWavelets==1.1.1\n",
        "!pip install git+https://github.com/fbcotter/pytorch_wavelets\n",
        "\n",
        "%cd /content\n",
        "!pip install git+https://github.com/eps696/aphantasia\n",
        "from aphantasia.image import to_valid_rgb, fft_image\n",
        "from aphantasia.utils import slice_imgs, derivat, pad_up_to, basename, file_list, img_list, img_read, txt_clean, plot_text, checkout, old_torch \n",
        "from aphantasia import transforms\n",
        "from aphantasia.progress_bar import ProgressIPy as ProgressBar\n",
        "\n",
        "clear_output()\n",
        "\n",
        "resume = False #@param {type:\"boolean\"}\n",
        "if resume:\n",
        "  resumed = files.upload()\n",
        "  params_pt = list(resumed.values())[0]\n",
        "  params_pt = torch.load(io.BytesIO(params_pt))\n",
        "  if isinstance(params_pt, list): params_pt = params_pt[0]\n",
        "\n",
        "def read_pt(file):\n",
        "  return torch.load(file).cuda()\n",
        "\n",
        "def ema(base, next, step):\n",
        "  scale_ma = 1. / (step + 1)\n",
        "  return next * scale_ma + base * (1.- scale_ma)\n",
        "\n",
        "def save_img(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def makevid(seq_dir, size=None):\n",
        "  out_sequence = seq_dir + '/%05d.jpg'\n",
        "  out_video = seq_dir + '.mp4'\n",
        "  print('.. generating video ..')\n",
        "  !ffmpeg -y -v warning -i $out_sequence -crf 20 $out_video\n",
        "  # moviepy.editor.ImageSequenceClip(img_list(seq_dir), fps=25).write_videofile(out_video, verbose=False)\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\n",
        "\n",
        "# Hardware check\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "import GPUtil as GPU\n",
        "gpu = GPU.getGPUs()[0] # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "!nvidia-smi -L\n",
        "print(\"GPU RAM {0:.0f}MB | Free {1:.0f}MB)\".format(gpu.memoryTotal, gpu.memoryFree))\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvpdy8BWGuM",
        "cellView": "form"
      },
      "source": [
        "#@title Upload text file\n",
        "\n",
        "#@markdown For non-English languages mark one of:\n",
        "\n",
        "multilang = False #@param {type:\"boolean\"}\n",
        "translate = False #@param {type:\"boolean\"}\n",
        "\n",
        "if translate:\n",
        "  !pip3 install googletrans==3.1.0a0\n",
        "  clear_output()\n",
        "  from googletrans import Translator\n",
        "  translator = Translator()\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "#@markdown (`multilang` = multi-language CLIP model, trained with ViT, \n",
        "#@markdown `translate` = Google translation, compatible with any visual model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Sj0fxmtw6K"
      },
      "source": [
        "### Settings\n",
        "Set the desired video resolution and `duration` (in sec).  \n",
        "Describe `style`, which you'd like to apply to the imagery.  \n",
        "Select CLIP visual `model` (results do vary!). I prefer ViT for consistency (and it's the only native multi-language option).  \n",
        "\n",
        "`align` option is about composition. `uniform` looks most adequate, `overscan` can make semi-seamless tileable texture.  \n",
        "`aug_transform` applies some augmentations, inhibiting image fragmentation & \"graffiti\" printing (slower, yet recommended).  \n",
        "Decrease `samples` if you face OOM (it's the main RAM eater).  \n",
        "Increasing `steps` will elaborate details and make tones smoother, but may start throwing texts like graffiti (and will obviously take more time).  \n",
        "`show_freq` controls preview frequency (doesn't affect the results; one can set it higher to speed up the process).  \n",
        "Tune `decay` (compositional softness) and `sharpness`, `colors` (saturation) and `contrast` as needed.  \n",
        "\n",
        "Experimental tricks:  \n",
        "`aug_noise` augmentation, `macro` (from 0 to 1) and `progressive_grow` (read more [here](https://github.com/eps696/aphantasia/issues/2)) may boost bigger forms, making composition less disperse.  \n",
        "`no_text` tries to remove \"graffiti\" by subtracting plotted text prompt. good start is \\~0.1.   \n",
        "`enhance` boosts training consistency (of simultaneous samples) and steps progress. good start is 0.1~0.2.  \n",
        "\n",
        "NB: `keep` parameter controls how well the next line/image generation follows the previous. By default X = 0, and every frame is produced independently (i.e. randomly initiated). \n",
        "Setting it higher starts each generation closer to the average of previous runs, effectively keeping macro compositions more similar and the transitions smoother. Safe values are < 0.5 (higher numbers may cause the imagery getting stuck). This behaviour depends on the input, so test with your prompts and see what's better in your case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s",
        "cellView": "form"
      },
      "source": [
        "#@title Generate\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/GDrive')\n",
        "# clipsDir = '/content/GDrive/MyDrive/T2I ' + dtNow.strftime(\"%Y-%m-%d %H%M\")\n",
        "\n",
        "style = \"\" #@param {type:\"string\"}\n",
        "sideX = 1280 #@param {type:\"integer\"}\n",
        "sideY = 720 #@param {type:\"integer\"}\n",
        "duration =  60#@param {type:\"integer\"}\n",
        "#@markdown > Config\n",
        "model = 'ViT-B/32' #@param ['ViT-B/16', 'ViT-B/32', 'RN101', 'RN50x16', 'RN50x4', 'RN50']\n",
        "align = 'uniform' #@param ['central', 'uniform', 'overscan']\n",
        "aug_transform = True #@param {type:\"boolean\"}\n",
        "keep = 0. #@param {type:\"number\"}\n",
        "#@markdown > Look\n",
        "decay = 1.5 #@param {type:\"number\"}\n",
        "colors = 1.5 #@param {type:\"number\"}\n",
        "contrast =  0.9#@param {type:\"number\"}\n",
        "sharpness = 0.3 #@param {type:\"number\"}\n",
        "#@markdown > Training\n",
        "steps = 200 #@param {type:\"integer\"}\n",
        "samples = 200 #@param {type:\"integer\"}\n",
        "learning_rate = .05 #@param {type:\"number\"}\n",
        "show_freq = 10 #@param {type:\"integer\"}\n",
        "#@markdown > Tricks\n",
        "aug_noise = 0.2 #@param {type:\"number\"}\n",
        "no_text = 0. #@param {type:\"number\"}\n",
        "enhance = 0. #@param {type:\"number\"}\n",
        "macro = 0.4 #@param {type:\"number\"}\n",
        "progressive_grow = False #@param {type:\"boolean\"}\n",
        "diverse = -enhance\n",
        "expand = abs(enhance)\n",
        "fps = 25\n",
        "if multilang: model = 'ViT-B/32' # sbert model is trained with ViT\n",
        "\n",
        "model_clip, _ = clip.load(model, jit=old_torch())\n",
        "modsize = model_clip.visual.input_resolution\n",
        "xmem = {'ViT-B/16':0.25, 'RN50':0.5, 'RN50x4':0.16, 'RN50x16':0.06, 'RN101':0.33}\n",
        "if model in xmem.keys():\n",
        "  samples = int(samples * xmem[model])\n",
        "\n",
        "def enc_text(txt):\n",
        "  if multilang:\n",
        "    model_lang = SentenceTransformer('clip-ViT-B-32-multilingual-v1').cuda()\n",
        "    emb = model_lang.encode([txt], convert_to_tensor=True, show_progress_bar=False).detach().clone()\n",
        "    del model_lang\n",
        "  else:\n",
        "    emb = model_clip.encode_text(clip.tokenize(txt).cuda())\n",
        "  return emb.detach().clone()\n",
        "\n",
        "if diverse != 0:\n",
        "  samples = int(samples * 0.5)\n",
        "        \n",
        "if aug_transform is True:\n",
        "  trform_f = transforms.transforms_fast\n",
        "  samples = int(samples * 0.95)\n",
        "else:\n",
        "  trform_f = transforms.normalize()\n",
        "\n",
        "text_file = list(uploaded)[0]\n",
        "texts = list(uploaded.values())[0].decode().split('\\n')\n",
        "texts = [tt.strip() for tt in texts if len(tt.strip())>0 and tt[0] != '#']\n",
        "print(' text file:', text_file)\n",
        "print(' total lines:', len(texts))\n",
        "\n",
        "if len(style) > 0:\n",
        "  print(' style:', style)\n",
        "  if translate:\n",
        "    style = translator.translate(style, dest='en').text\n",
        "    print(' translated to:', style) \n",
        "  txt_enc2 = enc_text(style)\n",
        "\n",
        "workdir = os.path.join(work_dir, basename(text_file))\n",
        "workdir += '-%s' % model if 'RN' in model.upper() else ''\n",
        "!rm -rf $workdir\n",
        "os.makedirs(workdir, exist_ok=True)\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "  \n",
        "# make init\n",
        "global params_start, params_ema\n",
        "params_shape = [1, 3, sideY, sideX//2+1, 2]\n",
        "params_start = torch.randn(*params_shape).cuda() # random init\n",
        "params_ema = 0.\n",
        "\n",
        "if resume is True:\n",
        "  # print(' resuming from', resumed)\n",
        "  # params, _, _ = fft_image([1, 3, sideY, sideX], resume = params_pt, sd=1.)\n",
        "  params_start = params_pt.cuda()\n",
        "  if keep > 0:\n",
        "    params_ema = params[0].detach()\n",
        "  torch.save(params_pt, os.path.join(workdir, '000-start.pt'))\n",
        "else:\n",
        "  torch.save(params_start, os.path.join(workdir, '000-start.pt'))\n",
        "\n",
        "torch.save(params_start, 'init.pt') # final init\n",
        "\n",
        "prev_enc = 0\n",
        "def process(txt, num):\n",
        "\n",
        "  global params_start\n",
        "  sd = 0.01\n",
        "  if keep > 0: sd = keep + (1-keep) * sd\n",
        "  params, image_f, _ = fft_image([1, 3, sideY, sideX], resume='init.pt', sd=sd, decay_power=decay)\n",
        "  image_f = to_valid_rgb(image_f, colors = colors)\n",
        "  \n",
        "  if progressive_grow is True:\n",
        "    lr1 = learning_rate * 2\n",
        "    lr0 = lr1 * 0.01\n",
        "  else:\n",
        "    lr0 = learning_rate\n",
        "  optimizer = torch.optim.AdamW(params, lr0, weight_decay=0.01, amsgrad=True)\n",
        "    \n",
        "  print(' topic: ', txt)\n",
        "  if translate:\n",
        "    txt = translator.translate(txt, dest='en').text\n",
        "    print(' translated to:', txt)\n",
        "  txt_enc = enc_text(txt)\n",
        "  if no_text > 0:\n",
        "      txt_plot = torch.from_numpy(plot_text(txt, modsize)/255.).unsqueeze(0).permute(0,3,1,2).cuda()\n",
        "      txt_plot_enc = model_clip.encode_image(txt_plot).detach().clone()\n",
        "  else: txt_plot_enc = None\n",
        "\n",
        "  out_name = '%03d-%s' % (num+1, txt_clean(txt))\n",
        "  tempdir = os.path.join(workdir, out_name)\n",
        "  !rm -rf $tempdir\n",
        "  os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "  pbar = ProgressBar(steps) #  // save_freq\n",
        "  for i in range(steps):\n",
        "    loss = 0\n",
        "    noise = aug_noise * torch.randn(1, 1, *params[0].shape[2:4], 1).cuda() if aug_noise > 0 else 0.\n",
        "    img_out = image_f(noise)\n",
        "    imgs_sliced = slice_imgs([img_out], samples, modsize, trform_f, align, macro=macro)\n",
        "    out_enc = model_clip.encode_image(imgs_sliced[-1])\n",
        "\n",
        "    loss -= torch.cosine_similarity(txt_enc, out_enc, dim=-1).mean()\n",
        "    if len(style) > 0:\n",
        "      loss -= 0.5 * torch.cosine_similarity(txt_enc2, out_enc, dim=-1).mean()\n",
        "    if no_text > 0:\n",
        "      loss += no_text * torch.cosine_similarity(txt_plot_enc, out_enc, dim=-1).mean()\n",
        "    if sharpness != 0: # mode = scharr|sobel|default\n",
        "      loss -= sharpness * derivat(img_out, mode='sobel')\n",
        "      # loss -= sharpness * derivat(img_sliced, mode='scharr')\n",
        "    if diverse != 0:\n",
        "      imgs_sliced = slice_imgs([image_f(noise)], samples, modsize, trform_f, align, macro=macro)\n",
        "      out_enc2 = model_clip.encode_image(imgs_sliced[-1])\n",
        "      loss += diverse * torch.cosine_similarity(out_enc, out_enc2, dim=-1).mean()\n",
        "      del out_enc2; torch.cuda.empty_cache()\n",
        "    if expand > 0:\n",
        "      global prev_enc\n",
        "      if i > 0:\n",
        "        loss += expand * torch.cosine_similarity(out_enc, prev_enc, dim=-1).mean()\n",
        "      prev_enc = out_enc.detach()\n",
        "    del img_out, imgs_sliced, out_enc; torch.cuda.empty_cache()\n",
        "\n",
        "    if progressive_grow is True:\n",
        "      lr_cur = lr0 + (i / steps) * (lr1 - lr0)\n",
        "      for g in optimizer.param_groups: \n",
        "        g['lr'] = lr_cur\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i % show_freq == 0:\n",
        "      with torch.no_grad():\n",
        "        img = image_f(contrast=contrast).cpu().numpy()[0]\n",
        "      if sharpness != 0:\n",
        "        img = img ** (1 + sharpness/2.) # empirical tone mapping\n",
        "      save_img(img, os.path.join(tempdir, '%05d.jpg' % (i // show_freq)))\n",
        "      outpic.clear_output()\n",
        "      with outpic:\n",
        "        display(Image('result.jpg'))\n",
        "      del img\n",
        "\n",
        "    pbar.upd()\n",
        "\n",
        "  if keep > 0:\n",
        "    global params_start, params_ema\n",
        "    params_ema = ema(params_ema, params[0].detach(), num+1)\n",
        "    torch.save((1-keep) * params_start + keep * params_ema, 'init.pt')\n",
        "\n",
        "  torch.save(params[0], '%s.pt' % os.path.join(workdir, out_name))\n",
        "  # shutil.copy(img_list(tempdir)[-1], os.path.join(workdir, '%s-%d.jpg' % (out_name, steps)))\n",
        "  # os.system('ffmpeg -v warning -y -i %s\\%%05d.jpg -codec nvenc \"%s.mp4\"' % (tempdir, os.path.join(workdir, out_name)))\n",
        "  # HTML(makevid(tempdir))\n",
        "\n",
        "for i, txt in enumerate(texts):\n",
        "    process(txt, i)\n",
        "\n",
        "vsteps = int(duration * fps / len(texts))\n",
        "tempdir = os.path.join(workdir, '_final')\n",
        "!rm -rf $tempdir\n",
        "os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "print(' rendering complete piece')\n",
        "ptfiles = file_list(workdir, 'pt')\n",
        "pbar = ProgressBar(vsteps * len(ptfiles))\n",
        "for px in range(len(ptfiles)):\n",
        "  params1 = read_pt(ptfiles[px])\n",
        "  params2 = read_pt(ptfiles[(px+1) % len(ptfiles)])\n",
        "\n",
        "  params, image_f, _ = fft_image([1, 3, sideY, sideX], resume=params1, sd=1., decay_power=decay)\n",
        "  image_f = to_valid_rgb(image_f, colors = colors)\n",
        "\n",
        "  for i in range(vsteps):\n",
        "    with torch.no_grad():\n",
        "      img = image_f((params2 - params1) * math.sin(1.5708 * i/vsteps)**2)[0].permute(1,2,0)\n",
        "      img = torch.clip(img*255, 0, 255).cpu().numpy().astype(np.uint8)\n",
        "    imageio.imsave(os.path.join(tempdir, '%05d.jpg' % (px * vsteps + i)), img)\n",
        "    _ = pbar.upd()\n",
        "\n",
        "HTML(makevid(tempdir))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rARq2n6LgVVp",
        "cellView": "form"
      },
      "source": [
        "#@markdown Run this, if you need to make the same video of another length (model must be the same)\n",
        "\n",
        "duration =  12#@param {type:\"integer\"}\n",
        "model = 'ViT-B/32' #@param ['ViT-B/32', 'RN101', 'RN50x4', 'RN50']\n",
        "colors = 1.5 #@param {type:\"number\"}\n",
        "fps = 25\n",
        "\n",
        "text_file = list(uploaded)[0]\n",
        "workdir = os.path.join(work_dir, basename(text_file))\n",
        "workdir += '-%s' % model if 'RN' in model.upper() else ''\n",
        "tempdir = os.path.join(workdir, '_final')\n",
        "!rm -rf $tempdir\n",
        "os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "print(' re-rendering final piece')\n",
        "ptfiles = file_list(workdir, 'pt')\n",
        "vsteps = int(duration * fps / (len(ptfiles)-1))\n",
        "\n",
        "ptest = torch.load(ptfiles[0])\n",
        "if isinstance(ptest, list): ptest = ptest[0]\n",
        "shape = [*ptest.shape[:3], (ptest.shape[3]-1)*2]\n",
        "\n",
        "pbar = ProgressBar(vsteps * len(ptfiles))\n",
        "for px in range(len(ptfiles)):\n",
        "  params1 = read_pt(ptfiles[px])\n",
        "  params2 = read_pt(ptfiles[(px+1) % len(ptfiles)])\n",
        "\n",
        "  params, image_f, _ = fft_image(shape, resume=params1, decay_power=decay)\n",
        "  image_f = to_valid_rgb(image_f, colors = colors)\n",
        "\n",
        "  for i in range(vsteps):\n",
        "    with torch.no_grad():\n",
        "      img = image_f((params2 - params1) * math.sin(1.5708 * i/vsteps)**2)[0].permute(1,2,0)\n",
        "      img = torch.clip(img*255, 0, 255).cpu().numpy().astype(np.uint8)\n",
        "    imageio.imsave(os.path.join(tempdir, '%05d.jpg' % (px * vsteps + i)), img)\n",
        "    _ = pbar.upd()\n",
        "\n",
        "HTML(makevid(tempdir))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}