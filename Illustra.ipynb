{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Illustra.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWe1IoH7X35"
      },
      "source": [
        "# Illustra: Multi-text to Image\r\n",
        "\r\n",
        "Based on [CLIP](https://github.com/openai/CLIP) + FFT from [Lucent](https://github.com/greentfrapp/lucent) // made by [eps696](https://github.com/eps696) [Vadim Epstein]  \r\n",
        "thanks to [Ryan Murdock](https://twitter.com/advadnoun), [Jonathan Fly](https://twitter.com/jonathanfly) for ideas\r\n",
        "\r\n",
        "## Features \r\n",
        "* **continuously processes phrase lists** (e.g. illustrating lyrics)\r\n",
        "* generates [FFT-encoded](https://github.com/greentfrapp/lucent/blob/master/lucent/optvis/param/spatial.py) image (massive detailed textures, a la deepdream)\r\n",
        "* fast convergence\r\n",
        "* undemanding for RAM - fullHD/4K and above\r\n",
        "* saving/loading FFT snapshots to resume processing\r\n",
        "* selectable CLIP model\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytcEMSKBtN-"
      },
      "source": [
        "**Run the cell below after each session restart**\r\n",
        "\r\n",
        "Mark `resume` and upload `.pt` file, if you're resuming from the saved params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title General setup\r\n",
        "\r\n",
        "import subprocess\r\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\r\n",
        "print(\"CUDA version:\", CUDA_version)\r\n",
        "\r\n",
        "if CUDA_version == \"10.0\":\r\n",
        "    torch_version_suffix = \"+cu100\"\r\n",
        "elif CUDA_version == \"10.1\":\r\n",
        "    torch_version_suffix = \"+cu101\"\r\n",
        "elif CUDA_version == \"10.2\":\r\n",
        "    torch_version_suffix = \"\"\r\n",
        "else:\r\n",
        "    torch_version_suffix = \"+cu110\"\r\n",
        "\r\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\r\n",
        "\r\n",
        "try: \r\n",
        "  !pip3 install googletrans==3.1.0a0\r\n",
        "  from googletrans import Translator, constants\r\n",
        "  translator = Translator()\r\n",
        "except: pass\r\n",
        "!pip install ftfy==5.8\r\n",
        "\r\n",
        "!apt-get -qq install ffmpeg\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/G', force_remount=True)\r\n",
        "gdir = !ls /G/\r\n",
        "gdir = '/G/%s/' % str(gdir[0])\r\n",
        "%cd $gdir\r\n",
        "work_dir = 'illustra'\r\n",
        "work_dir = gdir + work_dir + '/'\r\n",
        "import os\r\n",
        "os.makedirs(work_dir, exist_ok=True)\r\n",
        "%cd $work_dir\r\n",
        "\r\n",
        "import os\r\n",
        "import io\r\n",
        "import time\r\n",
        "import math\r\n",
        "# from math import exp\r\n",
        "import random\r\n",
        "import imageio\r\n",
        "import numpy as np\r\n",
        "import PIL\r\n",
        "# from skimage import exposure\r\n",
        "from base64 import b64encode\r\n",
        "import shutil\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "from IPython.display import HTML, Image, display, clear_output\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n",
        "import ipywidgets as ipy\r\n",
        "# import glob\r\n",
        "from google.colab import output, files\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "!pip install git+https://github.com/openai/CLIP.git\r\n",
        "import clip\r\n",
        "!pip install git+https://github.com/Po-Hsun-Su/pytorch-ssim\r\n",
        "import pytorch_ssim as ssim\r\n",
        "\r\n",
        "%cd /content\r\n",
        "!rm -rf aphantasia\r\n",
        "!git clone https://github.com/eps696/aphantasia\r\n",
        "%cd aphantasia/\r\n",
        "from clip_fft import to_valid_rgb, fft_image, slice_imgs, checkout\r\n",
        "from utils import pad_up_to, basename, file_list, img_list, img_read\r\n",
        "from progress_bar import ProgressIPy as ProgressBar\r\n",
        "\r\n",
        "clear_output()\r\n",
        "\r\n",
        "resume = False #@param {type:\"boolean\"}\r\n",
        "if resume:\r\n",
        "  resumed = files.upload()\r\n",
        "  params_pt = list(resumed.values())[0]\r\n",
        "  params_pt = torch.load(io.BytesIO(params_pt))\r\n",
        "\r\n",
        "def ema(base, next, step):\r\n",
        "    scale_ma = 1. / (step + 1)\r\n",
        "    return next * scale_ma + base * (1.- scale_ma)\r\n",
        "\r\n",
        "def save_img(img, fname=None):\r\n",
        "  img = np.array(img)[:,:,:]\r\n",
        "  img = np.transpose(img, (1,2,0))  \r\n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\r\n",
        "  if fname is not None:\r\n",
        "    imageio.imsave(fname, np.array(img))\r\n",
        "    imageio.imsave('result.jpg', np.array(img))\r\n",
        "\r\n",
        "def makevid(seq_dir, size=None):\r\n",
        "  out_sequence = seq_dir + '/%05d.jpg'\r\n",
        "  out_video = seq_dir + '.mp4'\r\n",
        "  !ffmpeg -y -v quiet -i $out_sequence $out_video\r\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\r\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\r\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\r\n",
        "\r\n",
        "!nvidia-smi -L\r\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvpdy8BWGuM",
        "cellView": "form"
      },
      "source": [
        "#@title Upload text file\r\n",
        "\r\n",
        "translate = False #@param {type:\"boolean\"}\r\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Sj0fxmtw6K"
      },
      "source": [
        "Set the desired video resolution and `duration` (in sec).  \r\n",
        "Select CLIP `model` (results do vary!). I prefer ViT for consistency.  \r\n",
        "\r\n",
        "Set `overscan` to produce semi-seamlessly tileable texture (when off, it's more centered).  \r\n",
        "Try adding `noise_scale` to explore some compositional changes.  \r\n",
        "Decrease `samples` if you face OOM (it's the main RAM eater).  \r\n",
        "Increasing `steps` will elaborate details and make tones smoother, but may start throwing texts like graffiti (and will obviously take more time).  \r\n",
        "`progressive_grow` may boost macro forms creation (especially with lower `learning_rate`, see more [here](https://github.com/eps696/aphantasia/issues/2)).   \r\n",
        "`show_freq` controls preview frequency (doesn't affect the results; one can set it higher to speed up process). \r\n",
        "\r\n",
        "`--keep X` parameter controls how well the next line/image generation follows the previous. By default X = 0, and every frame is produced independently (i.e. randomly initiated). \r\n",
        "Setting it higher starts each generation closer to the average of previous runs, effectively keeping macro compositions more similar and the transitions smoother. Safe values are < 0.5 (higher numbers may cause the imagery getting stuck). This behaviour depends on the input, so test with your prompts and see what's better in your case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s",
        "cellView": "form"
      },
      "source": [
        "#@title Generate\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/GDrive')\n",
        "# clipsDir = '/content/GDrive/MyDrive/T2I ' + dtNow.strftime(\"%Y-%m-%d %H%M\")\n",
        "\n",
        "sideX = 1280 #@param {type:\"integer\"}\n",
        "sideY = 720 #@param {type:\"integer\"}\n",
        "duration =  60#@param {type:\"integer\"}\n",
        "#@markdown > Config\n",
        "model = 'ViT-B/32' #@param ['ViT-B/32', 'RN101', 'RN50x4', 'RN50']\n",
        "overscan = False #@param {type:\"boolean\"}\n",
        "keep = 0. #@param {type:\"number\"}\n",
        "noise_scale = 0. #@param {type:\"number\"}\n",
        "contrast = 1. #@param {type:\"number\"}\n",
        "#@markdown > Training\n",
        "steps = 200 #@param {type:\"integer\"}\n",
        "samples = 200 #@param {type:\"integer\"}\n",
        "learning_rate = .05 #@param {type:\"number\"}\n",
        "progressive_grow = False #@param {type:\"boolean\"}\n",
        "show_freq = 10 #@param {type:\"integer\"}\n",
        "fps = 25\n",
        "\n",
        "model_clip, _ = clip.load(model)\n",
        "modsize = 288 if model == 'RN50x4' else 224\n",
        "xmem = {'RN50':0.5, 'RN50x4':0.16, 'RN101':0.33}\n",
        "if 'RN' in model:\n",
        "  samples = int(samples * xmem[model])\n",
        "\n",
        "norm_in = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "text_file = list(uploaded)[0]\n",
        "texts = list(uploaded.values())[0].decode().split('\\n')\n",
        "texts = [tt.strip() for tt in texts if len(tt.strip())>0 and tt[0] != '#']\n",
        "print(' text file:', text_file)\n",
        "print(' total lines:', len(texts))\n",
        "\n",
        "workdir = os.path.join(work_dir, basename(text_file))\n",
        "workdir += '-%s' % model if 'RN' in model.upper() else ''\n",
        "!rm -rf $workdir\n",
        "os.makedirs(workdir, exist_ok=True)\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "  \n",
        "# make init\n",
        "global params_start, params_ema\n",
        "params_shape = [1, 3, sideY, sideX//2+1, 2]\n",
        "params_start = torch.randn(*params_shape).cuda() # random init\n",
        "params_ema = 0.\n",
        "\n",
        "if resume is True:\n",
        "  # print(' resuming from', resumed)\n",
        "  params, _ = fft_image([1, 3, sideY, sideX], resume = params_pt)\n",
        "  if keep > 0:\n",
        "    params_ema = params[0].detach()\n",
        "  torch.save(params_pt, os.path.join(workdir, '000-start.pt'))\n",
        "else:\n",
        "  torch.save(params_start, os.path.join(workdir, '000-start.pt'))\n",
        "\n",
        "torch.save(params_start, 'init.pt') # final init\n",
        "\n",
        "def process(txt, num):\n",
        "\n",
        "  global params_start\n",
        "  params, image_f = fft_image([1, 3, sideY, sideX], resume='init.pt')\n",
        "  image_f = to_valid_rgb(image_f)\n",
        "  \n",
        "  if progressive_grow is True:\n",
        "    lr1 = learning_rate * 2\n",
        "    lr0 = lr1 * 0.01\n",
        "  else:\n",
        "    lr0 = learning_rate\n",
        "  optimizer = torch.optim.Adam(params, lr0)\n",
        "    \n",
        "  print(' ref text: ', txt)\n",
        "  if translate:\n",
        "    translator = Translator()\n",
        "    txt = translator.translate(txt, dest='en').text\n",
        "    print(' translated to:', txt)\n",
        "  tx = clip.tokenize(txt).cuda()\n",
        "  txt_enc = model_clip.encode_text(tx).detach().clone()\n",
        "  out_name = '%03d-%s' % (num+1, txt.translate(str.maketrans(dict.fromkeys(list(\"\\n',â€”|!?/:;\\\\\"), \"\"))).replace(' ', '_').replace('\"', ''))\n",
        "  tempdir = os.path.join(workdir, out_name)\n",
        "  !rm -rf $tempdir\n",
        "  os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "  pbar = ProgressBar(steps) #  // save_freq\n",
        "  for i in range(steps):\n",
        "    loss = 0\n",
        "    \n",
        "    noise = noise_scale * torch.randn(1, 1, *params[0].shape[2:4], 1).cuda() if noise_scale > 0 else 0.\n",
        "    img_out = image_f(noise)\n",
        "\n",
        "    imgs_sliced = slice_imgs([img_out], samples, modsize, norm_in, overscan=overscan, micro=None)\n",
        "    out_enc = model_clip.encode_image(imgs_sliced[-1])\n",
        "    loss -= torch.cosine_similarity(txt_enc, out_enc, dim=-1).mean()\n",
        "    del img_out, imgs_sliced, out_enc; torch.cuda.empty_cache()\n",
        "\n",
        "    if progressive_grow is True:\n",
        "      lr_cur = lr0 + (i / steps) * (lr1 - lr0)\n",
        "      for g in optimizer.param_groups: \n",
        "        g['lr'] = lr_cur\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i % show_freq == 0:\n",
        "      with torch.no_grad():\n",
        "        img = image_f(contrast=contrast).cpu().numpy()[0]\n",
        "      save_img(img, os.path.join(tempdir, '%05d.jpg' % (i // show_freq)))\n",
        "      outpic.clear_output()\n",
        "      with outpic:\n",
        "        display(Image('result.jpg'))\n",
        "      del img\n",
        "\n",
        "    pbar.upd()\n",
        "\n",
        "  if keep > 0:\n",
        "    global params_start, params_ema\n",
        "    params_ema = ema(params_ema, params[0].detach(), num+1)\n",
        "    torch.save((1-keep) * params_start + keep * params_ema, 'init.pt')\n",
        "\n",
        "  torch.save(params[0], '%s.pt' % os.path.join(workdir, out_name))\n",
        "  # shutil.copy(img_list(tempdir)[-1], os.path.join(workdir, '%s-%d.jpg' % (out_name, steps)))\n",
        "  # os.system('ffmpeg -v warning -y -i %s\\%%05d.jpg \"%s.mp4\"' % (tempdir, os.path.join(workdir, out_name)))\n",
        "  # HTML(makevid(tempdir))\n",
        "\n",
        "for i, txt in enumerate(texts):\n",
        "    process(txt, i)\n",
        "\n",
        "vsteps = int(duration * fps / len(texts))\n",
        "tempdir = os.path.join(workdir, '_final')\n",
        "!rm -rf $tempdir\n",
        "os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "def read_pt(file):\n",
        "  return torch.load(file).cuda()\n",
        "\n",
        "print(' rendering complete piece')\n",
        "ptfiles = file_list(workdir, 'pt')\n",
        "pbar = ProgressBar(vsteps * len(ptfiles))\n",
        "for px in range(len(ptfiles)):\n",
        "  params1 = read_pt(ptfiles[px])\n",
        "  params2 = read_pt(ptfiles[(px+1) % len(ptfiles)])\n",
        "\n",
        "  params, image_f = fft_image([1, 3, sideY, sideX], resume=params1)\n",
        "  image_f = to_valid_rgb(image_f)\n",
        "\n",
        "  for i in range(vsteps):\n",
        "    with torch.no_grad():\n",
        "      img = image_f((params2 - params1) * math.sin(1.5708 * i/vsteps)**2)[0].permute(1,2,0)\n",
        "      img = torch.clip(img*255, 0, 255).cpu().numpy().astype(np.uint8)\n",
        "    imageio.imsave(os.path.join(tempdir, '%05d.jpg' % (px * vsteps + i)), img)\n",
        "    _ = pbar.upd()\n",
        "\n",
        "HTML(makevid(tempdir))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}