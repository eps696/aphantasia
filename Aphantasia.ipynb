{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Aphantasia.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWe1IoH7X35"
      },
      "source": [
        "# Text to Image tool\r\n",
        "\r\n",
        "Based on [CLIP](https://github.com/openai/CLIP) + FFT from [Lucent](https://github.com/greentfrapp/lucent) // made by [eps696](https://github.com/eps696) [Vadim Epstein]  \r\n",
        "thanks to [Ryan Murdock](https://rynmurdock.github.io/), [Jonathan Fly](https://twitter.com/jonathanfly), [@tg-bomze](https://github.com/tg-bomze) \r\n",
        "\r\n",
        "## Features \r\n",
        "* complex requests:\r\n",
        "  * image and/or text as main prompts  \r\n",
        "   (composition similarity controlled with [SSIM](https://github.com/Po-Hsun-Su/pytorch-ssim) loss)\r\n",
        "  * additional text prompts for fine details and to subtract (avoid) topics\r\n",
        "  * criteria inversion (show \"the opposite\")\r\n",
        "\r\n",
        "* generates [FFT-encoded](https://github.com/greentfrapp/lucent/blob/master/lucent/optvis/param/spatial.py) image (massive detailed textures, a la deepdream)\r\n",
        "* ! fast convergence\r\n",
        "* ! undemanding for RAM - fullHD/4K and above\r\n",
        "* saving/loading FFT params to resume processing\r\n",
        "* can use both CLIP models at once (ViT and RN50)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytcEMSKBtN-"
      },
      "source": [
        "**Run the cell below after each session restart**\r\n",
        "\r\n",
        "Mark `resume` and upload `.pt` file to resume from saved params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title General setup\r\n",
        "\r\n",
        "import subprocess\r\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\r\n",
        "print(\"CUDA version:\", CUDA_version)\r\n",
        "\r\n",
        "if CUDA_version == \"10.0\":\r\n",
        "    torch_version_suffix = \"+cu100\"\r\n",
        "elif CUDA_version == \"10.1\":\r\n",
        "    torch_version_suffix = \"+cu101\"\r\n",
        "elif CUDA_version == \"10.2\":\r\n",
        "    torch_version_suffix = \"\"\r\n",
        "else:\r\n",
        "    torch_version_suffix = \"+cu110\"\r\n",
        "\r\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\r\n",
        "\r\n",
        "try: \r\n",
        "  !pip3 install googletrans==3.1.0a0\r\n",
        "  from googletrans import Translator, constants\r\n",
        "  # from pprint import pprint\r\n",
        "  translator = Translator()\r\n",
        "except: pass\r\n",
        "!pip install ftfy\r\n",
        "\r\n",
        "import os\r\n",
        "import io\r\n",
        "import time\r\n",
        "from math import exp\r\n",
        "import random\r\n",
        "import imageio\r\n",
        "import numpy as np\r\n",
        "import PIL\r\n",
        "from skimage import exposure\r\n",
        "from base64 import b64encode\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "from IPython.display import HTML, Image, display, clear_output\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n",
        "import ipywidgets as ipy\r\n",
        "# import glob\r\n",
        "from google.colab import output, files\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "!git clone https://github.com/openai/CLIP.git\r\n",
        "%cd /content/CLIP/\r\n",
        "import clip\r\n",
        "model_vit, _ = clip.load('ViT-B/32')\r\n",
        "\r\n",
        "workdir = '_out'\r\n",
        "tempdir = os.path.join(workdir, 'ttt')\r\n",
        "os.makedirs(tempdir, exist_ok=True)\r\n",
        "\r\n",
        "clear_output()\r\n",
        "\r\n",
        "resume = False #@param {type:\"boolean\"}\r\n",
        "if resume:\r\n",
        "  resumed = files.upload()\r\n",
        "  params_pt = list(resumed.values())[0]\r\n",
        "  # print(params_pt)\r\n",
        "  # print(params_pt.shape)\r\n",
        "\r\n",
        "###  FFT from Lucent library  https://github.com/greentfrapp/lucent\r\n",
        "\r\n",
        "def pixel_image(shape, sd=2.):\r\n",
        "  tensor = (torch.randn(*shape) * sd).cuda().requires_grad_(True)\r\n",
        "  return [tensor], lambda: tensor\r\n",
        "\r\n",
        "# From https://github.com/tensorflow/lucid/blob/master/lucid/optvis/param/spatial.py\r\n",
        "def rfft2d_freqs(h, w):\r\n",
        "  \"\"\"Computes 2D spectrum frequencies.\"\"\"\r\n",
        "  fy = np.fft.fftfreq(h)[:, None]\r\n",
        "  # when we have an odd input dimension we need to keep one additional frequency and later cut off 1 pixel\r\n",
        "  if w % 2 == 1:\r\n",
        "    fx = np.fft.fftfreq(w)[: w // 2 + 2]\r\n",
        "  else:\r\n",
        "    fx = np.fft.fftfreq(w)[: w // 2 + 1]\r\n",
        "  return np.sqrt(fx * fx + fy * fy)\r\n",
        "\r\n",
        "def fft_image(shape, sd=0.1, decay_power=1., contrast=1.):\r\n",
        "  batch, channels, h, w = shape\r\n",
        "  freqs = rfft2d_freqs(h, w)\r\n",
        "  init_val_size = (batch, channels) + freqs.shape + (2,) # 2 for imaginary and real components\r\n",
        "\r\n",
        "  if resume is True:\r\n",
        "    spectrum_real_imag_t = torch.load(io.BytesIO(params_pt))[0].cuda().requires_grad_(True)\r\n",
        "    print(' resuming from:', list(resumed)[0])\r\n",
        "  else:\r\n",
        "    spectrum_real_imag_t = (torch.randn(*init_val_size) * sd).cuda().requires_grad_(True)\r\n",
        "\r\n",
        "  scale = 1.0 / np.maximum(freqs, 1.0 / max(w, h)) ** decay_power\r\n",
        "  scale = torch.tensor(scale).float()[None, None, ..., None].cuda()\r\n",
        "\r\n",
        "  def inner():\r\n",
        "    scaled_spectrum_t = scale * spectrum_real_imag_t\r\n",
        "    image = torch.irfft(scaled_spectrum_t, 2, normalized=True, signal_sizes=(h, w))\r\n",
        "    image = image[:batch, :channels, :h, :w]\r\n",
        "    image = image * 1.33 / image.std() # keeping color contrast, empirical\r\n",
        "    image = image * contrast\r\n",
        "    return image\r\n",
        "  return [spectrum_real_imag_t], inner\r\n",
        "\r\n",
        "def to_valid_rgb(image_f, decorrelate=True):\r\n",
        "  def inner():\r\n",
        "    image = image_f()\r\n",
        "    if decorrelate:\r\n",
        "      image = _linear_decorrelate_color(image)\r\n",
        "    return torch.sigmoid(image)\r\n",
        "  return inner\r\n",
        "    \r\n",
        "def _linear_decorrelate_color(tensor):\r\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "  t_permute = tensor.permute(0,2,3,1)\r\n",
        "  t_permute = torch.matmul(t_permute, torch.tensor(color_correlation_normalized.T).to(device))\r\n",
        "  tensor = t_permute.permute(0,3,1,2)\r\n",
        "  return tensor\r\n",
        "\r\n",
        "color_correlation_svd_sqrt = np.asarray([[0.26, 0.09, 0.02],\r\n",
        "                                         [0.27, 0.00, -0.05],\r\n",
        "                                         [0.27, -0.09, 0.03]]).astype(\"float32\")\r\n",
        "max_norm_svd_sqrt = np.max(np.linalg.norm(color_correlation_svd_sqrt, axis=0))\r\n",
        "color_correlation_normalized = color_correlation_svd_sqrt / max_norm_svd_sqrt\r\n",
        "\r\n",
        "### Libs\r\n",
        "\r\n",
        "def slice_imgs(imgs, count, transform=None, uniform=False, micro=None):\r\n",
        "  def map(x, a, b):\r\n",
        "    return x * (b-a) + a\r\n",
        "\r\n",
        "  rnd_size = torch.rand(count)\r\n",
        "  if uniform is True:\r\n",
        "    rnd_offx = torch.rand(count)\r\n",
        "    rnd_offy = torch.rand(count)\r\n",
        "  else: # normal around center\r\n",
        "    rnd_offx = torch.clip(torch.randn(count) * 0.2 + 0.5, 0, 1) \r\n",
        "    rnd_offy = torch.clip(torch.randn(count) * 0.2 + 0.5, 0, 1)\r\n",
        "  \r\n",
        "  sz = [img.shape[2:] for img in imgs]\r\n",
        "  sz_min = [np.min(s) for s in sz]\r\n",
        "  if uniform is True:\r\n",
        "    sz = [[2*s[0], 2*s[1]] for s in list(sz)]\r\n",
        "    imgs = [pad_up_to(imgs[i], sz[i], type='centr') for i in range(len(imgs))]\r\n",
        "\r\n",
        "  sliced = []\r\n",
        "  for i, img in enumerate(imgs):\r\n",
        "    cuts = []\r\n",
        "    for c in range(count):\r\n",
        "      if micro is True: # both scales, micro mode\r\n",
        "        csize = map(rnd_size[c], 64, max(224, 0.25*sz_min[i])).int()\r\n",
        "      elif micro is False: # both scales, macro mode\r\n",
        "        csize = map(rnd_size[c], 0.5*sz_min[i], 0.98*sz_min[i]).int()\r\n",
        "      else: # single scale\r\n",
        "        csize = map(rnd_size[c], 64, 0.98*sz_min[i]).int()\r\n",
        "      offsetx = map(rnd_offx[c], 0, sz[i][1] - csize).int()\r\n",
        "      offsety = map(rnd_offy[c], 0, sz[i][0] - csize).int()\r\n",
        "      cut = img[:, :, offsety:offsety + csize, offsetx:offsetx + csize]\r\n",
        "      cut = torch.nn.functional.interpolate(cut, (224,224), mode='bicubic')\r\n",
        "      if transform is not None: \r\n",
        "        cut = transform(cut)\r\n",
        "      cuts.append(cut)\r\n",
        "    sliced.append(torch.cat(cuts, 0))\r\n",
        "  return sliced\r\n",
        "\r\n",
        "def makevid(seq_dir, size=None):\r\n",
        "  out_sequence = seq_dir + '/%03d.jpg'\r\n",
        "  out_video = seq_dir + '.mp4'\r\n",
        "  !ffmpeg -y -v warning -i $out_sequence $out_video\r\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\r\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\r\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\r\n",
        "\r\n",
        "# Tiles an array around two points, allowing for pad lengths greater than the input length\r\n",
        "# adapted from https://discuss.pytorch.org/t/symmetric-padding/19866/3\r\n",
        "def tile_pad(xt, padding):\r\n",
        "  h, w = xt.shape[-2:]\r\n",
        "  left, right, top, bottom = padding\r\n",
        "\r\n",
        "  def tile(x, minx, maxx):\r\n",
        "    rng = maxx - minx\r\n",
        "    mod = np.remainder(x - minx, rng)\r\n",
        "    out = mod + minx\r\n",
        "    return np.array(out, dtype=x.dtype)\r\n",
        "\r\n",
        "  x_idx = np.arange(-left, w+right)\r\n",
        "  y_idx = np.arange(-top, h+bottom)\r\n",
        "  x_pad = tile(x_idx, -0.5, w-0.5)\r\n",
        "  y_pad = tile(y_idx, -0.5, h-0.5)\r\n",
        "  xx, yy = np.meshgrid(x_pad, y_pad)\r\n",
        "  return xt[..., yy, xx]\r\n",
        "\r\n",
        "def pad_up_to(x, size, type='centr'):\r\n",
        "  sh = x.shape[2:][::-1]\r\n",
        "  if list(x.shape[2:]) == list(size): return x\r\n",
        "  padding = []\r\n",
        "  for i, s in enumerate(size[::-1]):\r\n",
        "    if 'side' in type.lower():\r\n",
        "      padding = padding + [0, s-sh[i]]\r\n",
        "    else: # centr\r\n",
        "      p0 = (s-sh[i]) // 2\r\n",
        "      p1 = s-sh[i] - p0\r\n",
        "      padding = padding + [p0,p1]\r\n",
        "  y = tile_pad(x, padding)\r\n",
        "  return y\r\n",
        "\r\n",
        "class ProgressBar(object):\r\n",
        "  def __init__(self, task_num=10):\r\n",
        "    self.pbar = ipy.IntProgress(min=0, max=task_num, bar_style='') # (value=0, min=0, max=max, step=1, description=description, bar_style='')\r\n",
        "    self.labl = ipy.Label()\r\n",
        "    display(ipy.HBox([self.pbar, self.labl]))\r\n",
        "    self.task_num = task_num\r\n",
        "    self.completed = 0\r\n",
        "    self.start()\r\n",
        "\r\n",
        "  def start(self, task_num=None):\r\n",
        "    if task_num is not None:\r\n",
        "      self.task_num = task_num\r\n",
        "    if self.task_num > 0:\r\n",
        "      self.labl.value = '0/{}'.format(self.task_num)\r\n",
        "    else:\r\n",
        "      self.labl.value = 'completed: 0, elapsed: 0s'\r\n",
        "    self.start_time = time.time()\r\n",
        "\r\n",
        "  def upd(self, *p, **kw):\r\n",
        "    self.completed += 1\r\n",
        "    elapsed = time.time() - self.start_time + 0.0000000000001\r\n",
        "    fps = self.completed / elapsed if elapsed>0 else 0\r\n",
        "    if self.task_num > 0:\r\n",
        "      finaltime = time.asctime(time.localtime(self.start_time + self.task_num * elapsed / float(self.completed)))\r\n",
        "      fin = ' end %s' % finaltime[11:16]\r\n",
        "      percentage = self.completed / float(self.task_num)\r\n",
        "      eta = int(elapsed * (1 - percentage) / percentage + 0.5)\r\n",
        "      self.labl.value = '{}/{}, rate {:.3g}s, time {}s, left {}s, {}'.format(self.completed, self.task_num, 1./fps, shortime(elapsed), shortime(eta), fin)\r\n",
        "    else:\r\n",
        "      self.labl.value = 'completed {}, time {}s, {:.1f} steps/s'.format(self.completed, int(elapsed + 0.5), fps)\r\n",
        "    self.pbar.value += 1\r\n",
        "    if self.completed == self.task_num: self.pbar.bar_style = 'success'\r\n",
        "    return \r\n",
        "    # return self.completed\r\n",
        "\r\n",
        "def time_days(sec):\r\n",
        "  return '%dd %d:%02d:%02d' % (sec/86400, (sec/3600)%24, (sec/60)%60, sec%60)\r\n",
        "def time_hrs(sec):\r\n",
        "  return '%d:%02d:%02d' % (sec/3600, (sec/60)%60, sec%60)\r\n",
        "def shortime(sec):\r\n",
        "  if sec < 60:\r\n",
        "    time_short = '%d' % (sec)\r\n",
        "  elif sec < 3600:\r\n",
        "    time_short  = '%d:%02d' % ((sec/60)%60, sec%60)\r\n",
        "  elif sec < 86400:\r\n",
        "    time_short  = time_hrs(sec)\r\n",
        "  else:\r\n",
        "    time_short = time_days(sec)\r\n",
        "  return time_short\r\n",
        "\r\n",
        "# from https://github.com/Po-Hsun-Su/pytorch-ssim\r\n",
        "\r\n",
        "def gaussian(window_size, sigma):\r\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\r\n",
        "    return gauss/gauss.sum()\r\n",
        "\r\n",
        "def create_window(window_size, channel):\r\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\r\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\r\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\r\n",
        "    return window\r\n",
        "\r\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\r\n",
        "  mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\r\n",
        "  mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\r\n",
        "  mu1_sq = mu1.pow(2)\r\n",
        "  mu2_sq = mu2.pow(2)\r\n",
        "  mu1_mu2 = mu1*mu2\r\n",
        "  sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\r\n",
        "  sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\r\n",
        "  sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\r\n",
        "  C1 = 0.01**2\r\n",
        "  C2 = 0.03**2\r\n",
        "  ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\r\n",
        "  if size_average:\r\n",
        "    return ssim_map.mean()\r\n",
        "  else:\r\n",
        "    return ssim_map.mean(1).mean(1).mean(1)\r\n",
        "\r\n",
        "class SSIM(torch.nn.Module):\r\n",
        "  def __init__(self, window_size = 11, size_average = True):\r\n",
        "    super(SSIM, self).__init__()\r\n",
        "    self.window_size = window_size\r\n",
        "    self.size_average = size_average\r\n",
        "    self.channel = 1\r\n",
        "    self.window = create_window(window_size, self.channel)\r\n",
        "\r\n",
        "  def forward(self, img1, img2):\r\n",
        "    (_, channel, _, _) = img1.size()\r\n",
        "    if channel == self.channel and self.window.data.type() == img1.data.type():\r\n",
        "      window = self.window\r\n",
        "    else:\r\n",
        "      window = create_window(self.window_size, channel)\r\n",
        "      if img1.is_cuda:\r\n",
        "        window = window.cuda(img1.get_device())\r\n",
        "      window = window.type_as(img1)\r\n",
        "      self.window = window\r\n",
        "      self.channel = channel\r\n",
        "    return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\r\n",
        "\r\n",
        "!nvidia-smi -L\r\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbJ9K4Cq8MtB"
      },
      "source": [
        "Type some `text` and/or upload some image to start.  \r\n",
        "`fine_details` input would make micro details follow that topic.  \r\n",
        "Put to `subtract` the topics, which you would like to avoid in the result.  \r\n",
        "*NB: more prompts = more memory! (handled by auto-decreasing `samples` amount, hopefully you don't need to act).*  \r\n",
        "`invert` the whole criteria, if you want to see \"the totally opposite\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvpdy8BWGuM",
        "cellView": "form"
      },
      "source": [
        "#@title Input\r\n",
        "\r\n",
        "text = \"test\" #@param {type:\"string\"}\r\n",
        "fine_details = \"\" #@param {type:\"string\"}\r\n",
        "subtract = \"\" #@param {type:\"string\"}\r\n",
        "translate = False #@param {type:\"boolean\"}\r\n",
        "invert = False #@param {type:\"boolean\"}\r\n",
        "upload_image = False #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "if translate:\r\n",
        "  text = translator.translate(text, dest='en').text\r\n",
        "if upload_image:\r\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Sj0fxmtw6K"
      },
      "source": [
        "`uniform` option produces semi-seamlessly tileable texture (when off, it's centered).  \r\n",
        "`sync` value adds SSIM loss between the output and input image (if there's one), allowing to \"redraw\" it with controlled similarity. \r\n",
        "\r\n",
        "Turn on `dual_model` to optimize with both CLIP models at once (eats more RAM!).  \r\n",
        "Decrease `samples` if you face OOM for higher resolutions (especially when several prompts are used with dual model).  \r\n",
        "Setting `steps` much higher (1000-..) will elaborate details and smoother tones, but may start throwing texts like graffiti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s",
        "cellView": "form"
      },
      "source": [
        "#@title Generate\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/GDrive')\n",
        "# clipsDir = '/content/GDrive/MyDrive/T2I ' + dtNow.strftime(\"%Y-%m-%d %H%M\")\n",
        "\n",
        "!rm -rf tempdir\n",
        "\n",
        "sideX = 1280 #@param {type:\"integer\"}\n",
        "sideY = 720 #@param {type:\"integer\"}\n",
        "#@markdown > Tweaks & tuning\n",
        "uniform = True #@param {type:\"boolean\"}\n",
        "dual_model = False #@param {type:\"boolean\"}\n",
        "sync =  0.3 #@param {type:\"number\"}\n",
        "#@markdown > Training\n",
        "steps = 200 #@param {type:\"integer\"}\n",
        "samples = 200 #@param {type:\"integer\"}\n",
        "learning_rate = .05 #@param {type:\"number\"}\n",
        "save_freq = 1 #@param {type:\"integer\"}\n",
        "\n",
        "if dual_model is True:\n",
        "  print(' using dual-model optimization')\n",
        "  model_rn, _ = clip.load('RN50')\n",
        "  samples = samples // 2\n",
        "if len(fine_details) > 0:\n",
        "  samples = int(samples * 0.9)\n",
        "if len(subtract) > 0:\n",
        "  samples = int(samples * 0.9)\n",
        "print(' using %d samples' % samples)\n",
        "\n",
        "norm_in = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "sign = 1. if invert is True else -1.\n",
        "\n",
        "if upload_image:\n",
        "  in_img = list(uploaded.values())[0]\n",
        "  print(' image:', list(uploaded)[0])\n",
        "  img_in = torch.from_numpy(imageio.imread(in_img).astype(np.float32)/255.).unsqueeze(0).permute(0,3,1,2).cuda()\n",
        "  in_sliced = slice_imgs([img_in], samples, transform=norm_in)[0]\n",
        "  img_enc = model_vit.encode_image(in_sliced).detach().clone()\n",
        "  if dual_model is True:\n",
        "    img_enc = torch.cat((img_enc, model_rn.encode_image(in_sliced).detach().clone()), 1)\n",
        "  if sync > 0:\n",
        "    ssim_loss = SSIM(window_size = 11)\n",
        "    img_in = F.interpolate(img_in, (sideY, sideX)).float()\n",
        "  else:\n",
        "    del img_in\n",
        "  del in_sliced; torch.cuda.empty_cache()\n",
        "\n",
        "if len(text) > 2:\n",
        "  print(' macro:', text)\n",
        "  if translate:\n",
        "    translator = Translator()\n",
        "    text = translator.translate(text, dest='en').text\n",
        "    print(' translated to:', text) \n",
        "  tx = clip.tokenize(text)\n",
        "  txt_enc = model_vit.encode_text(tx.cuda()).detach().clone()\n",
        "  if dual_model is True:\n",
        "    txt_enc = torch.cat((txt_enc, model_rn.encode_text(tx.cuda()).detach().clone()), 1)\n",
        "\n",
        "if len(fine_details) > 0:\n",
        "  print(' micro:', fine_details)\n",
        "  if translate:\n",
        "      translator = Translator()\n",
        "      fine_details = translator.translate(fine_details, dest='en').text\n",
        "      print(' translated to:', fine_details) \n",
        "  tx2 = clip.tokenize(fine_details)\n",
        "  txt_enc2 = model_vit.encode_text(tx2.cuda()).detach().clone()\n",
        "  if dual_model is True:\n",
        "      txt_enc2 = torch.cat((txt_enc2, model_rn.encode_text(tx2.cuda()).detach().clone()), 1)\n",
        "\n",
        "if len(subtract) > 0:\n",
        "  print(' without:', subtract)\n",
        "  if translate:\n",
        "      translator = Translator()\n",
        "      subtract = translator.translate(subtract, dest='en').text\n",
        "      print(' translated to:', subtract) \n",
        "  tx0 = clip.tokenize(subtract)\n",
        "  txt_enc0 = model_vit.encode_text(tx0.cuda()).detach().clone()\n",
        "  if dual_model is True:\n",
        "      txt_enc0 = torch.cat((txt_enc0, model_rn.encode_text(tx0.cuda()).detach().clone()), 1)\n",
        "\n",
        "shape = [1, 3, sideY, sideX]\n",
        "param_f = fft_image \n",
        "# param_f = pixel_image\n",
        "# learning_rate = 1.\n",
        "params, image_f = param_f(shape)\n",
        "image_f = to_valid_rgb(image_f)\n",
        "optimizer = torch.optim.Adam(params, learning_rate)\n",
        "\n",
        "def save_img(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def checkout(num):\n",
        "  with torch.no_grad():\n",
        "    img = image_f().cpu().numpy()[0]\n",
        "  save_img(img, os.path.join(tempdir, '%03d.jpg' % num))\n",
        "  outpic.clear_output()\n",
        "  with outpic:\n",
        "    display(Image('result.jpg'))\n",
        "\n",
        "def train(i):\n",
        "  loss = 0\n",
        "  img_out = image_f()\n",
        "\n",
        "  micro = False if len(fine_details) > 0 else None\n",
        "  imgs_sliced = slice_imgs([img_out], samples, norm_in, uniform=uniform, micro=micro)\n",
        "  out_enc = model_vit.encode_image(imgs_sliced[-1])\n",
        "  if dual_model is True: # use both clip models\n",
        "      out_enc = torch.cat((out_enc, model_rn.encode_image(imgs_sliced[-1])), 1)\n",
        "  if upload_image:\n",
        "      loss += sign * 100*torch.cosine_similarity(img_enc, out_enc, dim=-1).mean()\n",
        "  if len(text) > 0: # input text\n",
        "      loss += sign * 100*torch.cosine_similarity(txt_enc, out_enc, dim=-1).mean()\n",
        "  if len(subtract) > 0: # subtract text\n",
        "      loss += -sign * 100*torch.cosine_similarity(txt_enc0, out_enc, dim=-1).mean()\n",
        "  if sync > 0 and upload_image: # image composition sync\n",
        "      loss *= 1. + sync * (steps/(i+1) * ssim_loss(img_out, img_in) - 1)\n",
        "  if len(fine_details) > 0: # input text for micro details\n",
        "      imgs_sliced = slice_imgs([img_out], samples, norm_in, uniform=uniform, micro=True)\n",
        "      out_enc2 = model_vit.encode_image(imgs_sliced[-1])\n",
        "      if dual_model is True:\n",
        "          out_enc2 = torch.cat((out_enc2, model_rn.encode_image(imgs_sliced[-1])), 1)\n",
        "      loss += sign * 100*torch.cosine_similarity(txt_enc2, out_enc2, dim=-1).mean()\n",
        "      del out_enc2; torch.cuda.empty_cache()\n",
        "  del img_out, imgs_sliced, out_enc; torch.cuda.empty_cache()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  if i % save_freq == 0:\n",
        "    checkout(i // save_freq)\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "pbar = ProgressBar(steps)\n",
        "for i in range(steps):\n",
        "  train(i)\n",
        "  _ = pbar.upd()\n",
        "\n",
        "HTML(makevid(tempdir))\n",
        "torch.save(params, tempdir + '.pt')\n",
        "files.download(tempdir + '.pt')\n",
        "files.download(tempdir + '.mp4')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}